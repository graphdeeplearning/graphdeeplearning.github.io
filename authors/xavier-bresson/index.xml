<?xml version="1.0" encoding="utf-8" standalone="yes" ?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>NTU Graph Deep Learning Lab</title>
    <link>https://graphdeeplearning.github.io/authors/xavier-bresson/</link>
      <atom:link href="https://graphdeeplearning.github.io/authors/xavier-bresson/index.xml" rel="self" type="application/rss+xml" />
    <description>NTU Graph Deep Learning Lab</description>
    <generator>Source Themes Academic (https://sourcethemes.com/academic/)</generator><language>en-us</language><copyright>Xavier Bresson © 2020 · Made with &amp;hearts; by [Chaitanya Joshi](https://chaitjo.github.io/)</copyright><lastBuildDate>Fri, 12 Jun 2020 00:00:00 +0000</lastBuildDate>
    <image>
      <url>https://graphdeeplearning.github.io/images/icon_hu027d87ac1e37f4f802995042c9999554_21044_512x512_fill_lanczos_center_2.png</url>
      <title>NTU Graph Deep Learning Lab</title>
      <link>https://graphdeeplearning.github.io/authors/xavier-bresson/</link>
    </image>
    
    <item>
      <title>Learning TSP Requires Rethinking Generalization</title>
      <link>https://graphdeeplearning.github.io/publication/joshi-2020-learning/</link>
      <pubDate>Fri, 12 Jun 2020 00:00:00 +0000</pubDate>
      <guid>https://graphdeeplearning.github.io/publication/joshi-2020-learning/</guid>
      <description></description>
    </item>
    
    <item>
      <title>Benchmarking Graph Neural Networks</title>
      <link>https://graphdeeplearning.github.io/project/benchmark/</link>
      <pubDate>Tue, 03 Mar 2020 22:20:35 +0800</pubDate>
      <guid>https://graphdeeplearning.github.io/project/benchmark/</guid>
      <description></description>
    </item>
    
    <item>
      <title>Benchmarking Graph Neural Networks</title>
      <link>https://graphdeeplearning.github.io/publication/dwivedi-2020-benchmark/</link>
      <pubDate>Mon, 02 Mar 2020 00:00:00 +0000</pubDate>
      <guid>https://graphdeeplearning.github.io/publication/dwivedi-2020-benchmark/</guid>
      <description></description>
    </item>
    
    <item>
      <title>Free-hand Sketches</title>
      <link>https://graphdeeplearning.github.io/project/sketches/</link>
      <pubDate>Tue, 14 Jan 2020 16:19:27 +0800</pubDate>
      <guid>https://graphdeeplearning.github.io/project/sketches/</guid>
      <description>&lt;h2 id=&#34;representation-learning-for-sketches&#34;&gt;Representation Learning for Sketches&lt;/h2&gt;
&lt;p&gt;Human beings have been creating free-hand sketches, &lt;em&gt;i.e.&lt;/em&gt;, drawings without precise instruments, since &lt;a href=&#34;https://en.wikipedia.org/wiki/Cave_painting&#34;&gt;time immemorial&lt;/a&gt;.
Due to the popularity of touchscreen interfaces, machine learning using sketches has emerged as an interesting problem with a myriad of applications:
If we consider sketches as 2D images, we can throw them into off-the-shelf &lt;a href=&#34;https://arxiv.org/abs/1501.07873&#34;&gt;Convolutional Neural Networks (CNNs)&lt;/a&gt;.
While CNNs are designed for &lt;em&gt;static&lt;/em&gt; collections of pixels with &lt;em&gt;dense&lt;/em&gt; colors and textures,
sketches are usually an extremely &lt;em&gt;sparse&lt;/em&gt; sequences of strokes which capture high-level abstractions and ideas. &lt;a href=&#34;https://ai.googleblog.com/2017/04/teaching-machines-to-draw.html&#34;&gt;Recurrent Neural Networks (RNNs)&lt;/a&gt; stick out as a natural architecture for capturing this temporal nature of sketches.&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;&lt;em&gt;Structure vs. temporal order: can we have the best of both worlds?&lt;/em&gt;&lt;/p&gt;
&lt;/blockquote&gt;
&lt;h2 id=&#34;sketches-as-graphs&#34;&gt;Sketches as Graphs&lt;/h2&gt;
&lt;p&gt;We are working on a novel representation of free-hand sketches as &lt;strong&gt;sparsely-connected graphs&lt;/strong&gt;.
We assume that sketches are sets of curves and strokes, which are discretized by a set of points representing the graph nodes.
Each node encodes spatial, temporal and semantic information.
Thus, representing sketches with graphs offers a universal representation that can make use of both the sketch structure (like images) as well as temporal information (like stroke sequences).
To exploit these graph structures, we are developing &lt;strong&gt;Graph Neural Networks (GNNs)&lt;/strong&gt; based on the Transformer model &lt;a href=&#34;https://arxiv.org/abs/1706.03762&#34;&gt;[&lt;em&gt;Vaswani et al.&lt;/em&gt;, 2017]&lt;/a&gt;.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Multi-Graph Transformer for Free-Hand Sketch Recognition</title>
      <link>https://graphdeeplearning.github.io/publication/xu-2019-multi/</link>
      <pubDate>Tue, 24 Dec 2019 00:00:00 +0000</pubDate>
      <guid>https://graphdeeplearning.github.io/publication/xu-2019-multi/</guid>
      <description></description>
    </item>
    
    <item>
      <title>A Two-Step Graph Convolutional Decoder for Molecule Generation</title>
      <link>https://graphdeeplearning.github.io/publication/bresson-2019-two/</link>
      <pubDate>Sun, 01 Dec 2019 00:00:00 +0000</pubDate>
      <guid>https://graphdeeplearning.github.io/publication/bresson-2019-two/</guid>
      <description></description>
    </item>
    
    <item>
      <title>On Learning Paradigms for the Travelling Salesman Problem</title>
      <link>https://graphdeeplearning.github.io/publication/joshi-2019-learning/</link>
      <pubDate>Sun, 01 Dec 2019 00:00:00 +0000</pubDate>
      <guid>https://graphdeeplearning.github.io/publication/joshi-2019-learning/</guid>
      <description></description>
    </item>
    
    <item>
      <title>Graph Convolutional Neural Networks for Molecule Generation</title>
      <link>https://graphdeeplearning.github.io/talk/ipam-sept2019/</link>
      <pubDate>Mon, 23 Sep 2019 00:00:00 +0000</pubDate>
      <guid>https://graphdeeplearning.github.io/talk/ipam-sept2019/</guid>
      <description></description>
    </item>
    
    <item>
      <title>Combinatorial Optimization</title>
      <link>https://graphdeeplearning.github.io/project/combinatorial-optimization/</link>
      <pubDate>Tue, 17 Sep 2019 22:20:35 +0800</pubDate>
      <guid>https://graphdeeplearning.github.io/project/combinatorial-optimization/</guid>
      <description>&lt;h2 id=&#34;operations-research-and-combinatorial-problems&#34;&gt;Operations Research and Combinatorial Problems&lt;/h2&gt;
&lt;p&gt;&lt;strong&gt;&lt;a href=&#34;https://en.wikipedia.org/wiki/Operations_research&#34;&gt;Operations Research (OR)&lt;/a&gt;&lt;/strong&gt; started in the first world war as an initiative to use mathematics and computer science to assist military planners in their decisions.
Today, combinatorial optimization algorithms developed in the OR community form the backbone of the most important modern industries including transportation, logistics, scheduling, finance and supply chains.&lt;/p&gt;
&lt;p&gt;OR Problems are formulated as integer constrained optimization, &lt;em&gt;i.e.&lt;/em&gt;, with integral or binary variables (called decision variables).
While not all such problems are hard to solve (&lt;em&gt;e.g.&lt;/em&gt;, finding the shortest path between two locations), we concentrate on &lt;strong&gt;&lt;a href=&#34;https://en.wikipedia.org/wiki/Combinatorial_optimization&#34;&gt;Combinatorial (NP-Hard) problems&lt;/a&gt;&lt;/strong&gt;.
NP-Hard problems are &lt;em&gt;impossible&lt;/em&gt; to solve optimally at large scales as exhaustively searching for their solutions is beyond the limits of modern computers.
The &lt;a href=&#34;https://en.wikipedia.org/wiki/Travelling_salesman_problem&#34;&gt;Travelling Salesman Problem (TSP)&lt;/a&gt; and the &lt;a href=&#34;https://en.wikipedia.org/wiki/Minimum_spanning_tree&#34;&gt;Minimum Spanning Tree Problem (MST)&lt;/a&gt; are two of the most popular examples for such problems defined using graphs.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;tsp-gif.gif&#34; alt=&#34;TSP GIF&#34;&gt;
&lt;em&gt;TSP asks the following question: Given a list of cities and the distances between each pair of cities, what is the shortest possible route that visits each city and returns to the origin city? Formally, given a graph, one needs to search the space of permutations to find an optimal sequence of nodes, called a tour, with minimal total edge weights (tour length).&lt;/em&gt;&lt;/p&gt;
&lt;hr&gt;
&lt;h2 id=&#34;neural-combinatorial-optimization&#34;&gt;Neural Combinatorial Optimization&lt;/h2&gt;
&lt;p&gt;Solvers and heuristic algorithms developed in the OR community are able to solve classical problems such as TSP with up to millions of variables.
However, designing powerful and robust optimization algorithms requires significant &lt;strong&gt;specialized knowledge&lt;/strong&gt; and years of &lt;strong&gt;trial-and-error&lt;/strong&gt;, especially for understudied but high-impact problems arising in &lt;a href=&#34;https://arxiv.org/abs/2003.11755&#34;&gt;scientific discovery&lt;/a&gt; or &lt;a href=&#34;https://arxiv.org/abs/1911.05289&#34;&gt;computer architecture&lt;/a&gt;.
The state-of-the-art TSP solver, &lt;a href=&#34;http://www.math.uwaterloo.ca/tsp/concorde.html&#34;&gt;Concorde&lt;/a&gt;, leverages &lt;a href=&#34;https://www.youtube.com/watch?v=q8nQTNvCrjE&#34;&gt;over 50 years of research&lt;/a&gt; on linear programming, cutting plane algorithms and branch-and-bound.&lt;/p&gt;
&lt;div class=&#34;alert alert-note&#34;&gt;
  &lt;div&gt;
    At our lab, we&amp;rsquo;re working on &lt;strong&gt;automating&lt;/strong&gt; and &lt;strong&gt;augmenting&lt;/strong&gt; such expert intuition through Machine Learning [&lt;a href=&#34;https://arxiv.org/abs/1811.06128&#34;&gt;Bengio &lt;em&gt;et al.&lt;/em&gt;, 2018&lt;/a&gt;].
  &lt;/div&gt;
&lt;/div&gt;
&lt;p&gt;Since most problems are highly structured, heuristics take the form of rules or policies to make sequential decisions, &lt;em&gt;e.g.&lt;/em&gt;, determine the TSP tour one city at a time.
Our research uses deep neural networks to parameterize these policies and train them directly from problem instances.
In particular, &lt;strong&gt;Graph Neural Networks&lt;/strong&gt; are the perfect fit for the task because they naturally operate on the graph structure of these problems.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;pipeline.png&#34; alt=&#34;End-to-end pipeline&#34;&gt;
&lt;em&gt;A generic five-stage pipeline for end-to-end learning of combinatorial problems on graphs&lt;/em&gt;&lt;/p&gt;
&lt;hr&gt;
&lt;h2 id=&#34;why-study-tsp-in-particular&#34;&gt;Why study TSP in particular?&lt;/h2&gt;
&lt;p&gt;(1) The problem has an amazing history of serving as an &lt;strong&gt;engine of discovery&lt;/strong&gt; for applied mathematics, with several &lt;a href=&#34;https://en.wikipedia.org/wiki/John_von_Neumann&#34;&gt;legendary&lt;/a&gt; &lt;a href=&#34;https://en.wikipedia.org/wiki/Richard_E._Bellman&#34;&gt;computer&lt;/a&gt; &lt;a href=&#34;https://en.wikipedia.org/wiki/George_Dantzig&#34;&gt;scientists&lt;/a&gt; and &lt;a href=&#34;https://en.wikipedia.org/wiki/Edsger_W._Dijkstra&#34;&gt;mathematicians&lt;/a&gt; having a crack at it. Here&amp;rsquo;s an &lt;a href=&#34;https://www.youtube.com/watch?v=q8nQTNvCrjE&#34;&gt;amazing talk&lt;/a&gt; by William Cook, the co-inventor of the current state-of-the-art Concorde TSP solver.&lt;/p&gt;
&lt;p&gt;(2) TSP has been the focus of intense research in the combinatorial optimization community. If you come up with a new solver, &lt;em&gt;e.g.&lt;/em&gt;, a learning-driven solver, you &lt;em&gt;need&lt;/em&gt; to benchmark it on TSP. TSP’s &lt;strong&gt;multi-scale nature&lt;/strong&gt; makes it a challenging graph task which requires reasoning about both local node neighborhoods as well as global graph structure.&lt;/p&gt;
&lt;p&gt;(3) Learning-based approaches for heuristic algorithms have the potential to be a breakthrough for OR if they are able to  learn efficiently on small scale problems and then generalize robustly to larger instances.
However, such &lt;em&gt;scale-invariant&lt;/em&gt; generalization is an exciting and unsolved challenge, not just for TSP, but for machine learning as a whole.
&lt;strong&gt;Update:&lt;/strong&gt; We explore this in our &lt;a href=&#34;https://arxiv.org/abs/2006.07054&#34;&gt;latest paper&lt;/a&gt;!&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://imgs.xkcd.com/comics/travelling_salesman_problem.png&#34; alt=&#34;XKCD:TSP&#34;&gt;&lt;/p&gt;
&lt;hr&gt;
&lt;p&gt;At the same time, the more &lt;em&gt;profound&lt;/em&gt; motivation of using deep learning for combinatorial optimization is not to outperform classical approaches on well-studied problems.
Neural networks can be used as a general tool for tackling previously &lt;em&gt;un-encountered&lt;/em&gt; NP-hard problems, especially those that are &lt;strong&gt;non-trivial to design heuristics for&lt;/strong&gt; [&lt;a href=&#34;https://arxiv.org/abs/1611.09940&#34;&gt;Bello &lt;em&gt;et al.&lt;/em&gt;, 2016&lt;/a&gt;].
We are excited about recent applications of neural combinatorial optimization for &lt;a href=&#34;https://deepmind.com/blog/article/AlphaFold-Using-AI-for-scientific-discovery&#34;&gt;accelarating drug discovery&lt;/a&gt;, &lt;a href=&#34;https://arxiv.org/abs/1910.01578&#34;&gt;optimizing operating systems&lt;/a&gt; and &lt;a href=&#34;https://arxiv.org/abs/2004.10746&#34;&gt;designing computer chips&lt;/a&gt;.&lt;/p&gt;
&lt;hr&gt;
&lt;p&gt;&lt;strong&gt;P.S.&lt;/strong&gt; XB is organizing an exciting workshop at IPAM titled &lt;a href=&#34;http://www.ipam.ucla.edu/programs/workshops/deep-learning-and-combinatorial-optimization/&#34;&gt;&amp;ldquo;Deep Learning and Combinatorial Optimization&amp;rdquo;&lt;/a&gt;.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Quantum Chemistry</title>
      <link>https://graphdeeplearning.github.io/project/chemistry/</link>
      <pubDate>Tue, 17 Sep 2019 22:20:35 +0800</pubDate>
      <guid>https://graphdeeplearning.github.io/project/chemistry/</guid>
      <description></description>
    </item>
    
    <item>
      <title>Spatial Graph ConvNets</title>
      <link>https://graphdeeplearning.github.io/project/spatial-convnets/</link>
      <pubDate>Tue, 17 Sep 2019 22:20:35 +0800</pubDate>
      <guid>https://graphdeeplearning.github.io/project/spatial-convnets/</guid>
      <description>&lt;h2 id=&#34;non-euclidean-and-graph-structured-data&#34;&gt;Non-Euclidean and Graph-structured Data&lt;/h2&gt;
&lt;p&gt;Classic deep learning architectures such as &lt;a href=&#34;http://yann.lecun.com/exdb/publis/pdf/lecun-01a.pdf&#34;&gt;Convolutional Neural Networks (CNNs)&lt;/a&gt; and &lt;a href=&#34;https://www.bioinf.jku.at/publications/older/2604.pdf&#34;&gt;Recurrent Neural Networks (RNNs)&lt;/a&gt; require the input data domain to be regular, such as 2D or 3D Euclidean grids for Computer Vision and 1D lines for Natural Language Processing.&lt;/p&gt;
&lt;p&gt;However, real-world data beyond images and language tends to an underlying structure that is &lt;strong&gt;non-Euclidean&lt;/strong&gt;.
Such complex data commonly occurs in science and engineering, and can be modelled intuitively by &lt;strong&gt;heterogeneous graphs&lt;/strong&gt;.
Prominent examples include graphs of molecules, 3D meshes in computer graphics, social networks and biological networks.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;graph-data.png&#34; alt=&#34;Graph structured data&#34;&gt;&lt;/p&gt;
&lt;hr&gt;
&lt;h2 id=&#34;graph-neural-networks&#34;&gt;Graph Neural Networks&lt;/h2&gt;
&lt;p&gt;Obtaining insights from large and complex graph-structured datasets leads to an interesting challenge for machine learning architectures:
The popular CNN and RNN models need to be redesigned for handling non-Euclidean data, as they cannot leverage familiar regularities such as coordinate systems, vector space structure, or shift invariance.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Graph/Geometric Deep Learning&lt;/strong&gt; is an umbrella term for emerging techniques attempting to generalize deep neural networks to non-Euclidean domains such as graphs and manifolds [&lt;a href=&#34;https://arxiv.org/abs/1611.08097&#34;&gt;Bronstein &lt;em&gt;et al.&lt;/em&gt;, 2017&lt;/a&gt;].&lt;/p&gt;
&lt;div class=&#34;alert alert-note&#34;&gt;
  &lt;div&gt;
    We are interested to designing neural networks for arbitrary graphs in order to solve &lt;em&gt;generic&lt;/em&gt; graph problems, such as vertex classification, graph classification and graph generation.
  &lt;/div&gt;
&lt;/div&gt;
&lt;p&gt;These Graph Neural Network (GNN) architectures are used as backbones for challenging domain-specific applications in a myriad of domains, including &lt;a href=&#34;https://arxiv.org/abs/1704.01212&#34;&gt;chemistry&lt;/a&gt;, &lt;a href=&#34;https://arxiv.org/abs/1902.06673&#34;&gt;social networks&lt;/a&gt;, &lt;a href=&#34;https://arxiv.org/abs/1806.01973&#34;&gt;recommendations&lt;/a&gt; and &lt;a href=&#34;https://arxiv.org/abs/1611.08402&#34;&gt;computer graphics&lt;/a&gt;.&lt;/p&gt;
&lt;hr&gt;
&lt;h2 id=&#34;basic-formalism&#34;&gt;Basic Formalism&lt;/h2&gt;
&lt;p&gt;Each GNN layer computes $d$-dimensional representations for the nodes/edges of the graph through recursive neighborhood diffusion (&lt;em&gt;a.k.a.&lt;/em&gt; message passing), where each graph node gathers features from its neighbors to represent local graph structure.
Stacking $L$ GNN layers allows the network to build node representations from the &lt;strong&gt;$L$-hop neighborhood&lt;/strong&gt; of each node.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;gnn-layer.png&#34; alt=&#34;GNN Layer&#34;&gt;&lt;/p&gt;
&lt;p&gt;Let $h_i^{\ell}$ denote the feature vector at layer $\ell$ associated with node $i$.
The updated features $h_i^{\ell+1}$ at the next layer $\ell+1$ are obtained by applying non-linear transformations to the central feature vector $h_i^{\ell}$ and the feature vectors $h_{j}^{\ell}$ for all nodes $j$ in the neighborhood of node $i$ (defined by the graph structure).
This guarantees the transformation to build local reception fields, such as in standard ConvNets for computer vision, and be invariant to both graph size and vertex re-indexing.&lt;/p&gt;
&lt;p&gt;Thus, the most generic version of a feature vector $h_i^{\ell+1}$ at vertex $i$ at the next layer in the GNN is:
\begin{equation}
h_{i}^{\ell+1} =  f \left( \ h_i^{\ell} \  , \ { h_{j}^{\ell}: j \rightarrow i }  \ \right) ,
\end{equation}
where ${ j \rightarrow i }$ denotes the set of neighboring nodes $j$ pointed to node $i$, which can be replaced by ${ j \in \mathcal{N}_i }$, the set of neighbors of node $i$, if the graph is undirected.&lt;/p&gt;
&lt;hr&gt;
&lt;h2 id=&#34;classes-of-gnn-architectures&#34;&gt;Classes of GNN Architectures&lt;/h2&gt;
&lt;p&gt;In other words, a GNN is defined by a mapping $f$ taking as input a vector $h_i^{\ell}$&amp;ndash;the feature vector of the center vertex&amp;ndash;as well as an un-ordered set of vectors {${ h_{j}^{\ell}}$}&amp;ndash;the feature vectors of all neighboring vertices.
The arbitrary choice of the mapping $f$ defines an instantiation of a class of GNNs, &lt;i&gt;e.g.&lt;/i&gt;, &lt;a href=&#34;https://arxiv.org/abs/1609.02907&#34;&gt;GCN&lt;/a&gt;, &lt;a href=&#34;https://arxiv.org/abs/1706.02216&#34;&gt;GraphSage&lt;/a&gt;, &lt;a href=&#34;https://arxiv.org/abs/1810.00826&#34;&gt;GIN&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;As an illustration, here&amp;rsquo;s a simple-yet-effective Graph ConvNet from &lt;a href=&#34;https://arxiv.org/abs/1605.07736&#34;&gt;Sukhbaatar &lt;em&gt;et al.&lt;/em&gt;, 2016&lt;/a&gt;:
\begin{equation}
h_{i}^{\ell+1} = \text{ReLU} \Big( U^{\ell} h_{i}^{\ell} + \sum_{j \in \mathcal{N}_i} V^{\ell} h_{j}^{\ell} \Big),
\end{equation}
where $U^{\ell}, V^{\ell} \in \mathbb{R}^{d \times d}$ are the learnable parameters.&lt;/p&gt;
&lt;p&gt;In a &lt;a href=&#34;https://graphdeeplearning.github.io/publication/dwivedi-2020-benchmark/&#34;&gt;recent paper&lt;/a&gt; on benchmarking GNN architectures, we introduced &lt;strong&gt;block diagrams&lt;/strong&gt; to intuitively describe feature update equations such as the one above:&lt;/p&gt;













&lt;figure&gt;


  &lt;a data-fancybox=&#34;&#34; href=&#34;gcn-block.png&#34; &gt;
&lt;img data-src=&#34;gcn-block.png&#34; class=&#34;lazyload&#34; alt=&#34;&#34; width=&#34;45%&#34; &gt;&lt;/a&gt;



&lt;/figure&gt;

&lt;hr&gt;
&lt;h2 id=&#34;anisotropic-gnns&#34;&gt;Anisotropic GNNs&lt;/h2&gt;
&lt;p&gt;As graphs have no specific orientations (like up, down, left, right directions in images), message-passing layers such as Sukhbaatar&amp;rsquo;s Graph ConvNet are &lt;strong&gt;isotropic&lt;/strong&gt;, treating all neighbors as equally important.
However, this may not be true in general, &lt;em&gt;e.g.&lt;/em&gt;, in social network graphs, neighbors in the same community share different relationships and information compared to neighbors from separate communities.&lt;/p&gt;
&lt;p&gt;Isotropic GNNs can be upgraded to make the diffusion process &lt;strong&gt;anisotropic&lt;/strong&gt; through mechanisms which learn to weigh neighbors based on their relative importance.
For example, &lt;a href=&#34;https://arxiv.org/abs/1703.04826&#34;&gt;Marchegiani and Titov, 2017&lt;/a&gt; upgrade Graph ConvNets by introducing &lt;strong&gt;edge gating&lt;/strong&gt; for learning information flow on the graph structure for the task at hand:
\begin{equation}
h_{i}^{\ell+1} = \text{ReLU} \Big( U^{\ell} h_{i}^{\ell} + \sum_{j \in \mathcal{N}_i} \eta_{ij} \odot V^{\ell} h_{j}^{\ell} \Big),
\quad \text{where } \eta_{ij} = \sigma \big( A^{\ell} h_i^{\ell} + B^{\ell} h_j^{\ell} \big),
\end{equation}
$U^{\ell}, V^{\ell}, A^{\ell}, B^{\ell} \in \mathbb{R}^{d \times d}$ are the learnable parameters, $\sigma$ is the sigmoid function, $\odot$ is the element-wise product, and $\eta_{ij}$ act as edge gates.&lt;/p&gt;













&lt;figure&gt;


  &lt;a data-fancybox=&#34;&#34; href=&#34;gated-gcn-block.png&#34; &gt;
&lt;img data-src=&#34;gated-gcn-block.png&#34; class=&#34;lazyload&#34; alt=&#34;&#34; width=&#34;60%&#34; &gt;&lt;/a&gt;



&lt;/figure&gt;

&lt;p&gt;Other prominent approaches to introduce anisotropy into GNNs include &lt;a href=&#34;https://arxiv.org/abs/1710.10903&#34;&gt;GAT&lt;/a&gt;, which uses the &lt;a href=&#34;https://arxiv.org/abs/1706.03762&#34;&gt;attention mechanism&lt;/a&gt; from NLP, as well as &lt;a href=&#34;https://arxiv.org/abs/1611.08402&#34;&gt;MoNet&lt;/a&gt;, which relies on gaussian mixture models of graph connectivity.
&lt;strong&gt;Through &lt;a href=&#34;(https://graphdeeplearning.github.io/publication/dwivedi-2020-benchmark/)&#34;&gt;our benchmark&lt;/a&gt;, we found anisotropic aggregation to be a key property of powerful GNNs.&lt;/strong&gt;&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Graph Convolutional Neural Networks for Molecule Generation and Travelling Salesman Problem</title>
      <link>https://graphdeeplearning.github.io/talk/ipam-may2019/</link>
      <pubDate>Tue, 21 May 2019 00:00:00 +0000</pubDate>
      <guid>https://graphdeeplearning.github.io/talk/ipam-may2019/</guid>
      <description></description>
    </item>
    
    <item>
      <title>An Efficient Graph Convolutional Network Technique for the Travelling Salesman Problem</title>
      <link>https://graphdeeplearning.github.io/publication/joshi-2019-efficient/</link>
      <pubDate>Tue, 01 Jan 2019 00:00:00 +0000</pubDate>
      <guid>https://graphdeeplearning.github.io/publication/joshi-2019-efficient/</guid>
      <description></description>
    </item>
    
    <item>
      <title>GraphTSNE: A Visualization Technique for Graph-Structured Data</title>
      <link>https://graphdeeplearning.github.io/publication/leow-2019-graphtsne/</link>
      <pubDate>Tue, 01 Jan 2019 00:00:00 +0000</pubDate>
      <guid>https://graphdeeplearning.github.io/publication/leow-2019-graphtsne/</guid>
      <description></description>
    </item>
    
    <item>
      <title>Convolutional Neural Networks on Graphs</title>
      <link>https://graphdeeplearning.github.io/talk/ipam-feb2018/</link>
      <pubDate>Wed, 07 Feb 2018 00:00:00 +0000</pubDate>
      <guid>https://graphdeeplearning.github.io/talk/ipam-feb2018/</guid>
      <description></description>
    </item>
    
    <item>
      <title>An Experimental Comparison of Text Classification Techniques</title>
      <link>https://graphdeeplearning.github.io/publication/lakhotia-2018-experimental/</link>
      <pubDate>Mon, 01 Jan 2018 00:00:00 +0000</pubDate>
      <guid>https://graphdeeplearning.github.io/publication/lakhotia-2018-experimental/</guid>
      <description></description>
    </item>
    
    <item>
      <title>An Experimental Study of Neural Networks for Variable Graphs</title>
      <link>https://graphdeeplearning.github.io/publication/bresson-2018-experimental/</link>
      <pubDate>Mon, 01 Jan 2018 00:00:00 +0000</pubDate>
      <guid>https://graphdeeplearning.github.io/publication/bresson-2018-experimental/</guid>
      <description></description>
    </item>
    
    <item>
      <title>Residual Gated Graph ConvNets</title>
      <link>https://graphdeeplearning.github.io/publication/bresson-2017-residual/</link>
      <pubDate>Sun, 01 Jan 2017 00:00:00 +0000</pubDate>
      <guid>https://graphdeeplearning.github.io/publication/bresson-2017-residual/</guid>
      <description></description>
    </item>
    
  </channel>
</rss>
