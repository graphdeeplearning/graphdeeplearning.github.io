[{"authors":["chaitanya.joshi"],"categories":null,"content":"Chaitanya Joshi is a Research Assistant under Dr. Xavier Bresson at NTU, Singapore. His current research focuses on the emerging field of Graph Deep Learning and its applications for Operations Research and Combinatorial Optimization.\nHe graduated from NTU in 2019 as the Valedictorian of his cohort with a BEng in Computer Science and a specialization in Artificial Intelligence. He is passionate about building data-driven solutions for real-world problems, and has 3+ years of experience doing the same at companies and research labs in Singapore and Switzerland. He has co-authored 3 US patent applications and 4 research papers at top Machine Learning conferences such as NeurIPS and ICLR.\n","date":1578989967,"expirydate":-62135596800,"kind":"taxonomy","lang":"en","lastmod":1578989967,"objectID":"691c63b6f91f7d89a8ed5b8a3523bcdb","permalink":"https://graphdeeplearning.github.io/authors/chaitanya.joshi/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/authors/chaitanya.joshi/","section":"authors","summary":"Chaitanya Joshi is a Research Assistant under Dr. Xavier Bresson at NTU, Singapore. His current research focuses on the emerging field of Graph Deep Learning and its applications for Operations Research and Combinatorial Optimization.\nHe graduated from NTU in 2019 as the Valedictorian of his cohort with a BEng in Computer Science and a specialization in Artificial Intelligence. He is passionate about building data-driven solutions for real-world problems, and has 3+ years of experience doing the same at companies and research labs in Singapore and Switzerland.","tags":null,"title":"Chaitanya Joshi","type":"authors"},{"authors":["peng.xu"],"categories":null,"content":"Peng Xu is a Postdoctoral Scholar working with Dr. Xavier Bresson at NTU, Singapore. His current research focuses on sketch-based human-computer interaction and representation learning for sketches, as well as computer vision. He received his PhD degree from Pattern Recognition and Intelligent System Laboratory (PRIS) in Beijing University of Posts and Telecommunications (BUPT), supervised by Prof. Jun Guo. During his PhD, he was a visiting student in sketchX Lab, Queen Mary University of London (QMUL) and the National Laboratory of Pattern Recognition (NLPR), Institute of Automation, Chinese Academy of Sciences (CASIA).\n","date":1578989967,"expirydate":-62135596800,"kind":"taxonomy","lang":"en","lastmod":1578989967,"objectID":"7fa2d94b2d77f96cfdfdfc3856b8e331","permalink":"https://graphdeeplearning.github.io/authors/peng.xu/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/authors/peng.xu/","section":"authors","summary":"Peng Xu is a Postdoctoral Scholar working with Dr. Xavier Bresson at NTU, Singapore. His current research focuses on sketch-based human-computer interaction and representation learning for sketches, as well as computer vision. He received his PhD degree from Pattern Recognition and Intelligent System Laboratory (PRIS) in Beijing University of Posts and Telecommunications (BUPT), supervised by Prof. Jun Guo. During his PhD, he was a visiting student in sketchX Lab, Queen Mary University of London (QMUL) and the National Laboratory of Pattern Recognition (NLPR), Institute of Automation, Chinese Academy of Sciences (CASIA).","tags":null,"title":"Peng Xu","type":"authors"},{"authors":["xavier.bresson"],"categories":null,"content":"Xavier Bresson (PhD 2005, EPFL, Switzerland) is Associate Professor in Computer Science at NTU, Singapore. He is a leading researcher in the field of Graph Deep Learning, a new framework that combines graph theory and deep learning techniques to tackle complex data domains in natural language processing, computer vision, combinatorial optimization, quantum chemistry, physics, neuroscience, genetics and social networks. In 2016, he received the highly competitive Singaporean NRF Fellowship of $2.5M to develop these deep learning techniques. He was also awarded several research grants in the U.S. and Hong Kong. As a leading researcher in the field, he has published more than 60 peer-reviewed papers in the leading journals and conference proceedings in machine learning, including articles in NeurIPS, ICML, ICLR, CVPR, JMLR. He has organized several international workshops and tutorials on AI and deep learning in collaboration with Facebook, NYU and Imperial such as the 2019 and 2018 UCLA workshops, the 2017 CVPR tutorial and the 2017 NeurIPS tutorial. He has been teaching undergraduate, graduate and industrial courses in AI and deep learning since 2014 at EPFL (Switzerland), NTU (Singapore) and UCLA (U.S.).\n","date":1578989967,"expirydate":-62135596800,"kind":"taxonomy","lang":"en","lastmod":1578989967,"objectID":"66dece2ea6e76c6fa5b8df971df974bb","permalink":"https://graphdeeplearning.github.io/authors/xavier.bresson/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/authors/xavier.bresson/","section":"authors","summary":"Xavier Bresson (PhD 2005, EPFL, Switzerland) is Associate Professor in Computer Science at NTU, Singapore. He is a leading researcher in the field of Graph Deep Learning, a new framework that combines graph theory and deep learning techniques to tackle complex data domains in natural language processing, computer vision, combinatorial optimization, quantum chemistry, physics, neuroscience, genetics and social networks. In 2016, he received the highly competitive Singaporean NRF Fellowship of $2.","tags":null,"title":"Xavier Bresson","type":"authors"},{"authors":["victor.getty"],"categories":null,"content":"Victor Getty is a Research Assistant under Dr. Xavier Bresson at NTU, Singapore, working on applications of Graph Neural Networks to Quantum Chemistry. He graduated from NTU in 2018 with a BSc in Mathematics.\n","date":1568730035,"expirydate":-62135596800,"kind":"taxonomy","lang":"en","lastmod":1568730035,"objectID":"acdd5de4788d7280f9e5f46c129d30af","permalink":"https://graphdeeplearning.github.io/authors/victor.getty/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/authors/victor.getty/","section":"authors","summary":"Victor Getty is a Research Assistant under Dr. Xavier Bresson at NTU, Singapore, working on applications of Graph Neural Networks to Quantum Chemistry. He graduated from NTU in 2018 with a BSc in Mathematics.","tags":null,"title":"Victor Getty","type":"authors"},{"authors":["vijay.dwivedi"],"categories":null,"content":"Vijay Dwivedi is a first year PhD student in Machine Learning at NTU, Singapore supervised by Dr. Xavier Bresson. His primary interest is developing Deep Learning algorithms on graph-structured data and their applications to domains such as quantum chemistry, social networks, etc.\nBefore starting his PhD, Vijay worked with Dr. Bresson as a Research Assistant in the same lab. He has a background in Computer Science (BTech) from MNNIT Allahabad, where he explored the fields of Natural Language Processing and Multi-Modal Systems.\n","date":1568730035,"expirydate":-62135596800,"kind":"taxonomy","lang":"en","lastmod":1568730035,"objectID":"f5be614b2e9cf93a9fded6e1b42a6cb7","permalink":"https://graphdeeplearning.github.io/authors/vijay.dwivedi/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/authors/vijay.dwivedi/","section":"authors","summary":"Vijay Dwivedi is a first year PhD student in Machine Learning at NTU, Singapore supervised by Dr. Xavier Bresson. His primary interest is developing Deep Learning algorithms on graph-structured data and their applications to domains such as quantum chemistry, social networks, etc.\nBefore starting his PhD, Vijay worked with Dr. Bresson as a Research Assistant in the same lab. He has a background in Computer Science (BTech) from MNNIT Allahabad, where he explored the fields of Natural Language Processing and Multi-Modal Systems.","tags":null,"title":"Vijay Prakash Dwivedi","type":"authors"},{"authors":["david.low"],"categories":null,"content":"David Low is currently a PhD student under School of Computer Science \u0026amp; Engineering, NTU supervised by Associate Professor Xavier Bresson. His current research focuses on Deep Learning and its applications for Natural Language Processing.\nBefore starting his PhD, he cofounded two startups and worked as a data scientist at Infocomm Development Authority, Singapore. In 2016, he represented Singapore and National University of Singapore (NUS) in Data Science Game at France and clinched top spot among teams from Asia and America.\nThroughout his career, David has engaged in data science projects ranging from banking, telco, e-commerce to insurance industry. Some of his works including sales forecast modeling, mineral deposits prediction and process optimization had won him awards in several machine learning competitions. Earlier in his career, David was involved in research collaborations with Carnegie Mellon University (CMU) and Massachusetts Institute of Technology (MIT) on separate projects funded by National Research Foundation and SMART.\n","date":-62135596800,"expirydate":-62135596800,"kind":"taxonomy","lang":"en","lastmod":-62135596800,"objectID":"5715af036d64cf572dba0bdcbad83c85","permalink":"https://graphdeeplearning.github.io/authors/david.low/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/authors/david.low/","section":"authors","summary":"David Low is currently a PhD student under School of Computer Science \u0026amp; Engineering, NTU supervised by Associate Professor Xavier Bresson. His current research focuses on Deep Learning and its applications for Natural Language Processing.\nBefore starting his PhD, he cofounded two startups and worked as a data scientist at Infocomm Development Authority, Singapore. In 2016, he represented Singapore and National University of Singapore (NUS) in Data Science Game at France and clinched top spot among teams from Asia and America.","tags":null,"title":"David Low Jia Wei","type":"authors"},{"authors":["Peng Xu","Chaitanya Joshi","Xavier Bresson"],"categories":["Applications"],"content":"","date":1578989967,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1578989967,"objectID":"7d87904d26648130a430d73db48d16e4","permalink":"https://graphdeeplearning.github.io/project/sketches/","publishdate":"2020-01-14T16:19:27+08:00","relpermalink":"/project/sketches/","section":"project","summary":"Representation learning for sketches and drawings through graphs with geometric and temporal information.","tags":["Deep Learning","Computer Vision","Graph Neural Networks","Applications","Sketches"],"title":"Free-hand Sketches","type":"project"},{"authors":["Peng Xu","Chaitanya Joshi","Xavier Bresson"],"categories":null,"content":"","date":1577145600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1577145600,"objectID":"df5c75cf16edaf9f688f7becec58bf91","permalink":"https://graphdeeplearning.github.io/publication/xu-2019-multi/","publishdate":"2020-01-14T16:08:24+08:00","relpermalink":"/publication/xu-2019-multi/","section":"publication","summary":"Learning meaningful representations of free-hand sketches remains a challenging task given the signal sparsity and the high-level abstraction of sketches. Existing techniques have focused on exploiting either the static nature of sketches with Convolutional Neural Networks (CNNs) or the temporal sequential property with Recurrent Neural Networks (RNNs). In this work, we propose a new representation of sketches as multiple sparsely connected graphs. We design a novel Graph Neural Network (GNN), the Multi-Graph Transformer (MGT), for learning representations of sketches from multiple graphs which simultaneously capture global and local geometric stroke structures, as well as temporal information. We report extensive numerical experiments on a sketch recognition task to demonstrate the performance of the proposed approach. Particularly, MGT applied on 414k sketches from Google QuickDraw: (i) achieves small recognition gap to the CNN-based performance upper bound (72.80% vs. 74.22%), and (ii) outperforms all RNN-based models by a significant margin. To the best of our knowledge, this is the first work proposing to represent sketches as graphs and apply GNNs for sketch recognition.","tags":["Deep Learning","Computer Vision","Graph Neural Networks","Sketches","Transformer"],"title":"Multi-Graph Transformer for Free-Hand Sketch Recognition","type":"publication"},{"authors":["Xavier Bresson","Thomas Laurent"],"categories":null,"content":"","date":1575158400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1575158400,"objectID":"683f5c06eabce515341c98c57c59c2fc","permalink":"https://graphdeeplearning.github.io/publication/bresson-2019-two/","publishdate":"2019-09-17T00:46:25.621256Z","relpermalink":"/publication/bresson-2019-two/","section":"publication","summary":"We propose a simple auto-encoder framework for molecule generation. The molecular graph is first encoded into a continuous latent representation , which is then decoded back to a molecule. The encoding process is easy, but the decoding process remains challenging. In this work, we introduce a simple two-step decoding process. In a first step, a fully connected neural network uses the latent vector  to produce a molecular formula, for example CO (one carbon and two oxygen atoms). In a second step, a graph convolutional neural network uses the same latent vector  to place bounds between the atoms that were produced in the first step (for example a double bound will be placed between the carbon and each of the oxygens). This two-step process, in which a bag of atoms is first generated, and then assembled, provides a simple framework that allows us to develop an efficient molecule auto-encoder. Numerical experiments on basic tasks such as novelty, uniqueness, validity and optimized chemical property for the 250k ZINC molecules demonstrate the performances of the proposed system. Particularly, we achieve the highest reconstruction rate of 90.5%, improving the previous rate of 76.7%. We also report the best property improvement results when optimization is constrained by the molecular distance between the original and generated molecules.","tags":["Deep Learning","Graph Neural Networks","Chemistry","Molecule Generation"],"title":"A Two-Step Graph Convolutional Decoder for Molecule Generation","type":"publication"},{"authors":["Chaitanya Joshi","Thomas Laurent","Xavier Bresson"],"categories":null,"content":"","date":1575158400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1575158400,"objectID":"03a8196783d781b067826437fd6b3933","permalink":"https://graphdeeplearning.github.io/publication/joshi-2019-learning/","publishdate":"2019-09-17T00:46:25.621256Z","relpermalink":"/publication/joshi-2019-learning/","section":"publication","summary":"We explore the impact of learning paradigms on training deep neural networks for the Travelling Salesman Problem. We design controlled experiments to train supervised learning (SL) and reinforcement learning (RL) models on fixed graph sizes up to 100 nodes, and evaluate them on variable sized graphs up to 500 nodes. Beyond not needing labelled data, our results reveal favorable properties of RL over SL: RL training leads to better emergent generalization to variable graph sizes and is a key component for learning scale-invariant solvers for novel combinatorial problems.","tags":["Deep Learning","Graph Neural Networks","Operations Research","Combinatorial Optimization","Travelling Salesman Problem"],"title":"On Learning Paradigms for the Travelling Salesman Problem","type":"publication"},{"authors":["Chaitanya Joshi"],"categories":null,"content":"","date":1571702400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1571702400,"objectID":"b55be0b37ff4d3d6c1af1e03fc805809","permalink":"https://graphdeeplearning.github.io/talk/informs-oct2019/","publishdate":"2019-09-18T15:41:13+08:00","relpermalink":"/talk/informs-oct2019/","section":"talk","summary":"The most famous NP-hard combinatorial problem today, the Travelling Salesman Problem, is intractable to solve optimally at large scale. In practice, existing techniques such as Concorde can efficiently solve TSP up to thousands of nodes. This talk introduces a recent line of work from the deep learning community to directly ‘learn’ good heuristics for TSP using neural networks. Our approach uses Graph ConvNets to operate on the graph structure of problem instances and is highly parallelizable, making it a promising direction for learning combinatorial optimization at large scale.","tags":["Deep Learning","Graph Neural Networks","Talks","Operations Research","Combinatorial Optimization"],"title":"Graph Neural Networks for the Travelling Salesman Problem","type":"talk"},{"authors":["Xavier Bresson"],"categories":null,"content":"","date":1569196800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1569196800,"objectID":"c1ea4c6aeb1e382b0b7d95f63756c2ae","permalink":"https://graphdeeplearning.github.io/talk/ipam-sept2019/","publishdate":"2019-09-18T15:41:13+08:00","relpermalink":"/talk/ipam-sept2019/","section":"talk","summary":"In this talk, I will discuss a graph convolutional neural network architecture for the molecule generation task. The proposed approach consists of two steps. First, a graph ConvNet is used to auto-encode molecules in one-shot. Second, beam search is applied to the output of neural networks to produce a valid chemical solution. Numerical experiments demonstrate the performances of this learning system.","tags":["Deep Learning","Graph Neural Networks","Talks","Chemistry"],"title":"Graph Convolutional Neural Networks for Molecule Generation","type":"talk"},{"authors":["Chaitanya Joshi","Xavier Bresson"],"categories":["Applications"],"content":"Operations Research and Combinatorial Problems Operations Research (OR) started in the first world war as an initiative to use mathematics and computer science to assist military planners in their decisions. Nowadays it is widely used in the industry, including but not limited to transportation, supply chain, energy, finance, and scheduling.\nOR Problems are formulated as integer constrained optimization, i.e., with integral or binary variables (called decision variables). While not all such problems are hard to solve (e.g., finding the shortest path between two locations), we concentrate on Combinatorial (NP-Hard) problems. NP-Hard problems are impossible to solve optimally at large scales as exhaustively searching for their solutions is beyond the limits of modern computers. The Travelling Salesman Problem (TSP) and the Minimum Spanning Tree Problem (MST) are two of the most popular examples for such problems defined using graphs.\nTSP asks the following question: Given a list of cities and the distances between each pair of cities, what is the shortest possible route that visits each city and returns to the origin city? Formally, given a graph, one needs to search the space of permutations to find an optimal sequence of nodes, called a tour, with minimal total edge weights (tour length).\n Neural Combinatorial Optimization Although handcrafted heuristic algorithms are able to solve problems such as TSP with up to a million variables, designing good heuristics often requires significant specialized knowledge and years of trial-and-error. The state-of-the-art TSP solver, Concorde, leverages over 50 years of research on linear programming, cutting plane algorithms and branch-and-bound. At our lab, we're working on automating and augmenting such expert intuition through Machine Learning [Bengio et al., 2018].\nSince most problems are highly structured, heuristics take the form of rules or policies to make sequential decisions, e.g., determine the TSP tour one city at a time. Our research uses deep neural networks to parameterize these policies and train them directly from problem instances. In particular, Graph Neural Networks are the perfect fit for the task because they naturally operate on the graph structure of these problems.\n","date":1568730035,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1568730035,"objectID":"24dbe575959522cae5b8371ef64eccb7","permalink":"https://graphdeeplearning.github.io/project/combinatorial-optimization/","publishdate":"2019-09-17T22:20:35+08:00","relpermalink":"/project/combinatorial-optimization/","section":"project","summary":"Scalable deep learning systems for practical NP-Hard combinatorial problems such as the TSP.","tags":["Deep Learning","Graph Neural Networks","Applications","Operations Research","Combinatorial Optimization"],"title":"Combinatorial Optimization","type":"project"},{"authors":["Victor Getty","Xavier Bresson"],"categories":["Applications"],"content":"","date":1568730035,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1568730035,"objectID":"a96c5f414cb8f29ed26411f228728935","permalink":"https://graphdeeplearning.github.io/project/chemistry/","publishdate":"2019-09-17T22:20:35+08:00","relpermalink":"/project/chemistry/","section":"project","summary":"Predictive models for chemical synthesis, structure and properties using deep learning.","tags":["Deep Learning","Graph Neural Networks","Applications","Chemistry"],"title":"Quantum Chemistry","type":"project"},{"authors":["Vijay Prakash Dwivedi","Chaitanya Joshi","Xavier Bresson"],"categories":["Models"],"content":"Non-Euclidean and Graph-structured Data Classic deep learning architectures such as Convolutional Neural Networks (CNNs) and Recurrent Neural Networks (RNNs) require the input data domain to be regular, such as 2D or 3D Euclidean grids for Computer Vision and 1D lines for Natural Language Processing.\nHowever, most real-world data beyond images and language has an underlying structure that is non-Euclidean. Such complex data commonly occurs in science and engineering, and can be modelled by heterogeneous graphs. Examples include chemical graphs, computer graphics, social networks, genetics, neuroscience, and sensor networks.\n Graph Neural Networks Graph-structured data can be large and complex (in the case of social networks, on the scale of billions), and is a natural target for machine learning applications. However, designing models for learning from non-Euclidean data is challenging as there are no familiar properties such as coordinate systems, vector space structure, or shift invariance.\nGraph/Geometric Deep Learning is an umbrella term for emerging techniques attempting to generalize deep neural networks to non-Euclidean domains such as graphs and manifolds [Bronstein et al., 2017]. We are interested to designing neural networks for graphs of arbitrary topologies and structures in order to solve generic graph problems, such as vertex classification, graph classification, graph regression, and graph generation. These Graph Neural Network (GNN) architectures are used as backbones for challenging domain-specific applications in chemistry, social networks or graphics.\nGNNs iteratively build representations of graphs through recursive neighborhood aggregation (or message passing), where each graph node gathers features from its neighbors to represent local graph structure.\n","date":1568730035,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1568730035,"objectID":"60ff349a5ab17be5dbb8131562dd38b3","permalink":"https://graphdeeplearning.github.io/project/spatial-convnets/","publishdate":"2019-09-17T22:20:35+08:00","relpermalink":"/project/spatial-convnets/","section":"project","summary":"Graph Neural Network architectures for inductive representation learning on arbitrary graphs.","tags":["Deep Learning","Graph Neural Networks","Models","Spatial Graph ConvNets"],"title":"Spatial Graph ConvNets","type":"project"},{"authors":["Xavier Bresson"],"categories":null,"content":"","date":1558396800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1558396800,"objectID":"bfaf594a2cddd16a686088705a7d702b","permalink":"https://graphdeeplearning.github.io/talk/ipam-may2019/","publishdate":"2019-09-18T15:41:13+08:00","relpermalink":"/talk/ipam-may2019/","section":"talk","summary":"In this talk, I will discuss how to apply graph convolutional neural networks to quantum chemistry and operational research. The same high-level paradigm can be applied to generate new molecules with optimized chemical properties and to solve the Travelling Salesman Problem. The proposed approach consists of two steps. First, a graph ConvNet is used to auto-encode molecules and estimate TSP solutions in one-shot. Second, beam search is applied to the output of neural networks to produce a valid chemical or combinatorial solution. Numerical experiments demonstrate the performances of this learning system.","tags":["Deep Learning","Graph Neural Networks","Talks","Operations Research","Combinatorial Optimization","Chemistry"],"title":"Graph Convolutional Neural Networks for Molecule Generation and Travelling Salesman Problem","type":"talk"},{"authors":["Chaitanya Joshi","Thomas Laurent","Xavier Bresson"],"categories":null,"content":"","date":1546300800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1546300800,"objectID":"b87bd8cbb6b6d49da1ad5087b786f634","permalink":"https://graphdeeplearning.github.io/publication/joshi-2019-efficient/","publishdate":"2019-09-17T00:46:25.621256Z","relpermalink":"/publication/joshi-2019-efficient/","section":"publication","summary":"This paper introduces a new learning-based approach for approximately solving the Travelling Salesman Problem on 2D Euclidean graphs. We use deep Graph Convolutional Networks to build efficient TSP graph representations and output tours in a non-autoregressive manner via highly parallelized beam search. Our approach outperforms all recently proposed autoregressive deep learning techniques in terms of solution quality, inference speed and sample efficiency for problem instances of fixed graph sizes. In particular, we reduce the average optimality gap from 0.52% to 0.01% for 50 nodes, and from 2.26% to 1.39% for 100 nodes. Finally, despite improving upon other learning-based approaches for TSP, our approach falls short of standard Operations Research solvers.","tags":["Deep Learning","Graph Neural Networks","Operations Research","Combinatorial Optimization","Travelling Salesman Problem"],"title":"An Efficient Graph Convolutional Network Technique for the Travelling Salesman Problem","type":"publication"},{"authors":["Yao Yang Leow","Thomas Laurent","Xavier Bresson"],"categories":null,"content":"","date":1546300800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1546300800,"objectID":"7f44a49a4449c77f77725f074b04a600","permalink":"https://graphdeeplearning.github.io/publication/leow-2019-graphtsne/","publishdate":"2019-09-17T00:46:25.621256Z","relpermalink":"/publication/leow-2019-graphtsne/","section":"publication","summary":"We present GraphTSNE, a novel visualization technique for graph-structured data based on t-SNE. The growing interest in graph-structured data increases the importance of gaining human insight into such datasets by means of visualization. However, among the most popular visualization techniques, classical t-SNE is not suitable on such datasets because it has no mechanism to make use of information from graph connectivity. On the other hand, standard graph visualization techniques, such as Laplacian Eigenmaps, have no mechanism to make use of information from node features. Our proposed method GraphTSNE is able to produce visualizations which account for both graph connectivity and node features. It is based on scalable and unsupervised training of a graph convolutional network on a modified t-SNE loss. By assembling a suite of evaluation metrics, we demonstrate that our method produces desirable visualizations on three benchmark datasets.","tags":["Deep Learning","Graph Neural Networks","Graph Visualization"],"title":"GraphTSNE: A Visualization Technique for Graph-Structured Data","type":"publication"},{"authors":["Xavier Bresson"],"categories":null,"content":"","date":1517961600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1517961600,"objectID":"4af49a26a13058d786bcf0d7f41536d1","permalink":"https://graphdeeplearning.github.io/talk/ipam-feb2018/","publishdate":"2019-09-18T15:41:13+08:00","relpermalink":"/talk/ipam-feb2018/","section":"talk","summary":"Convolutional neural networks have greatly improved state-of-the-art performances in computer vision and speech analysis tasks, due to its high ability to extract multiple levels of representations of data. In this talk, we are interested in generalizing convolutional neural networks from low-dimensional regular grids, where image, video and speech are represented, to high-dimensional irregular domains, such as social networks, telecommunication networks, or words' embedding. We present a formulation of convolutional neural networks on graphs in the context of spectral graph theory, which provides the necessary mathematical background and efficient numerical schemes to design fast localized convolutional filters on graphs. Numerical experiments demonstrate the ability of the system to learn local stationary features on graphs.","tags":["Deep Learning","Graph Neural Networks","Talks","Spatial Graph ConvNets","Spectral Graph ConvNets"],"title":"Convolutional Neural Networks on Graphs","type":"talk"},{"authors":["Suyash Lakhotia","Xavier Bresson"],"categories":null,"content":"","date":1514764800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1514764800,"objectID":"d2066092208d9e8d9f6b1dd67e569dba","permalink":"https://graphdeeplearning.github.io/publication/lakhotia-2018-experimental/","publishdate":"2019-09-17T00:46:25.621256Z","relpermalink":"/publication/lakhotia-2018-experimental/","section":"publication","summary":"Text classification is the task of labeling text data from a predetermined set of thematic labels. It has become of increasing importance in recent years as we generate large volumes of data and require the ability to search through these vast datasets with flexible queries. However, manually labeling text data is an extremely tedious task that is prone to human error. Thus, text classification has become a key focus of machine learning research, with the goal of producing models that are more efficient and accurate than traditional methods. The objective of this work is to rigorously compare the performance of current text classification techniques, from standard SVM-based, statistical and multilayer perceptron (MLP) models to recently enhanced deep learning models such as convolutional neural networks and their fusion with graph theory. Extensive numerical experiments on three major text classification datasets (Rotten Tomatoes Sentence Polarity, 20 Newsgroups and Reuters Corpus Volume 1) revealed two results. First, graph convolutional neural networks perform with greater or similar test accuracy when compared to standard convolutional neural networks, SVM-based models and statistical baseline models. Second, and more surprisingly, simpler MLP models still outperform recent deep learning techniques despite having fewer parameters. This implies that either benchmark datasets like RCV1 containing more than 420,000 documents from 52 classes are not large enough or the representation of text data as tf-idf document vectors is not expressive enough.","tags":["Deep Learning","Graph Neural Networks","Natural Language Processing","Text Classification"],"title":"An Experimental Comparison of Text Classification Techniques","type":"publication"},{"authors":["Xavier Bresson","Thomas Laurent"],"categories":null,"content":"","date":1514764800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1514764800,"objectID":"41ebcf5e731d32acdfc83406ac3056f2","permalink":"https://graphdeeplearning.github.io/publication/bresson-2018-experimental/","publishdate":"2019-09-17T00:46:25.619962Z","relpermalink":"/publication/bresson-2018-experimental/","section":"publication","summary":"Graph-structured data such as social networks, functional brain networks, chemical molecules have brought the interest in generalizing deep learning techniques to graph domains. In this work, we propose an empirical study of neural networks for graphs with variable size and connectivity. We rigorously compare several graph recurrent neural networks (RNNs) and graph convolutional neural networks (ConvNets) to solve two fundamental and representative graph problems, subgraph matching and graph clustering. Numerical results show that graph ConvNets are 3-17% more accurate and 1.5-4x faster than graph RNNs. Interestingly, graph ConvNets are also 36% more accurate than non-learning (variational) techniques. The benefit of such study is to show that complex architectures like LSTM is not useful in the context of graph neural networks, but one should favour architectures with minimal inner structures, such as locality, weight sharing, index invariance, multi-scale, gates and residuality, to design efficient novel neural network models for applications like drugs design, genes analysis and particle physics.","tags":["Deep Learning","Graph Neural Networks","Spatial Graph ConvNets"],"title":"An Experimental Study of Neural Networks for Variable Graphs","type":"publication"},{"authors":["Xavier Bresson","Thomas Laurent"],"categories":null,"content":"","date":1483228800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1483228800,"objectID":"49bbf3fe527899f70dfc3309fc0edbb0","permalink":"https://graphdeeplearning.github.io/publication/bresson-2017-residual/","publishdate":"2019-09-17T00:46:25.620574Z","relpermalink":"/publication/bresson-2017-residual/","section":"publication","summary":"Graph-structured data such as social networks, functional brain networks, gene regulatory networks, communications networks have brought the interest in generalizing deep learning techniques to graph domains. In this paper, we are interested to design neural networks for graphs with variable length in order to solve learning problems such as vertex classification, graph classification, graph regression, and graph generative tasks. Most existing works have focused on recurrent neural networks (RNNs) to learn meaningful representations of graphs, and more recently new convolutional neural networks (ConvNets) have been introduced. In this work, we want to compare rigorously these two fundamental families of architectures to solve graph learning tasks. We review existing graph RNN and ConvNet architectures, and propose natural extension of LSTM and ConvNet to graphs with arbitrary size. Then, we design a set of analytically controlled experiments on two basic graph problems, ie subgraph matching and graph clustering, to test the different architectures. Numerical results show that the proposed graph ConvNets are 3-17% more accurate and 1.5-4x faster than graph RNNs. Graph ConvNets are also 36% more accurate than variational (non-learning) techniques. Finally, the most effective graph ConvNet architecture uses gated edges and residuality. Residuality plays an essential role to learn multi-layer architectures as they provide a 10% gain of performance.","tags":["Deep Learning","Graph Neural Networks","Spatial Graph ConvNets"],"title":"Residual Gated Graph ConvNets","type":"publication"}]