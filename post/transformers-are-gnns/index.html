<!DOCTYPE html>
<html lang="en-us">

<head>

  <meta charset="utf-8">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <meta http-equiv="X-UA-Compatible" content="IE=edge">
  <meta name="generator" content="Source Themes Academic 4.7.0">

  

  
  
  
  
  
    
    
    
  
  

  <meta name="author" content="Chaitanya Joshi">

  
  
  
    
  
  <meta name="description" content="Engineer friends often ask me: Graph Deep Learning sounds great, but are there any big commercial success stories? Is it being deployed in practical applications?
Besides the obvious ones&ndash;recommendation systems at Pinterest, Alibaba and Twitter&ndash;a slightly nuanced success story is the Transformer architecture, which has taken the NLP industry by storm.
Through this post, I want to establish links between Graph Neural Networks (GNNs) and Transformers. I&rsquo;ll talk about the intuitions behind model architectures in the NLP and GNN communities, make connections using equations and figures, and discuss how we could work together to drive progress.">

  
  <link rel="alternate" hreflang="en-us" href="https://graphdeeplearning.github.io/post/transformers-are-gnns/">

  


  
  
  
  <meta name="theme-color" content="#2962ff">
  

  
  
  
  <script src="/js/mathjax-config.js"></script>
  

  
  
  
  
    
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/academicons/1.8.6/css/academicons.min.css" integrity="sha256-uFVgMKfistnJAfoCUQigIl+JfUaP47GrRKjf6CTPVmw=" crossorigin="anonymous">
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.11.2/css/all.min.css" integrity="sha256-+N4/V/SbAFiW1MPBCXnfnP9QSN3+Keu+NlB+0ev/YKQ=" crossorigin="anonymous">
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/fancybox/3.5.7/jquery.fancybox.min.css" integrity="sha256-Vzbj7sDDS/woiFS3uNKo8eIuni59rjyNGtXfstRzStA=" crossorigin="anonymous">

    
    
    
      
    
    
      
      
        
          <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/9.15.10/styles/github.min.css" crossorigin="anonymous" title="hl-light">
          <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/9.15.10/styles/dracula.min.css" crossorigin="anonymous" title="hl-dark" disabled>
        
      
    

    
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/leaflet/1.5.1/leaflet.css" integrity="sha256-SHMGCYmST46SoyGgo4YR/9AlK1vf3ff84Aq9yK4hdqM=" crossorigin="anonymous">
    

    

    
    
      

      
      

      
    
      

      
      

      
    
      

      
      

      
    
      

      
      

      
    
      

      
      

      
    
      

      
      

      
    
      

      
      

      
    
      

      
      

      
    
      

      
      

      
    
      

      
      

      
    
      

      
      

      
        <script src="https://cdnjs.cloudflare.com/ajax/libs/lazysizes/5.1.2/lazysizes.min.js" integrity="sha256-Md1qLToewPeKjfAHU1zyPwOutccPAm5tahnaw7Osw0A=" crossorigin="anonymous" async></script>
      
    
      

      
      

      
    
      

      
      

      
    
      

      
      

      
        <script src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml.js" integrity="" crossorigin="anonymous" async></script>
      
    
      

      
      

      
    

  

  
  
  
  <link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Montserrat:400,700%7CRoboto:400,400italic,700%7CRoboto+Mono&display=swap">
  

  
  
  
  
  <link rel="stylesheet" href="/css/academic.css">

  




  


  

  <link rel="manifest" href="/index.webmanifest">
  <link rel="icon" type="image/png" href="/images/icon_hu027d87ac1e37f4f802995042c9999554_21044_32x32_fill_lanczos_center_2.png">
  <link rel="apple-touch-icon" type="image/png" href="/images/icon_hu027d87ac1e37f4f802995042c9999554_21044_192x192_fill_lanczos_center_2.png">

  <link rel="canonical" href="https://graphdeeplearning.github.io/post/transformers-are-gnns/">

  
  
  
  
  
    
  
  
  <meta property="twitter:card" content="summary_large_image">
  
  <meta property="twitter:site" content="@xbresson">
  <meta property="twitter:creator" content="@xbresson">
  
  <meta property="og:site_name" content="NTU Graph Deep Learning Lab">
  <meta property="og:url" content="https://graphdeeplearning.github.io/post/transformers-are-gnns/">
  <meta property="og:title" content="Transformers are Graph Neural Networks | NTU Graph Deep Learning Lab">
  <meta property="og:description" content="Engineer friends often ask me: Graph Deep Learning sounds great, but are there any big commercial success stories? Is it being deployed in practical applications?
Besides the obvious ones&ndash;recommendation systems at Pinterest, Alibaba and Twitter&ndash;a slightly nuanced success story is the Transformer architecture, which has taken the NLP industry by storm.
Through this post, I want to establish links between Graph Neural Networks (GNNs) and Transformers. I&rsquo;ll talk about the intuitions behind model architectures in the NLP and GNN communities, make connections using equations and figures, and discuss how we could work together to drive progress."><meta property="og:image" content="https://graphdeeplearning.github.io/post/transformers-are-gnns/featured.jpeg">
  <meta property="twitter:image" content="https://graphdeeplearning.github.io/post/transformers-are-gnns/featured.jpeg"><meta property="og:locale" content="en-us">
  
    
      <meta property="article:published_time" content="2020-02-12T16:08:39&#43;08:00">
    
    <meta property="article:modified_time" content="2020-02-12T16:08:39&#43;08:00">
  

  


    






  






<script type="application/ld+json">
{
  "@context": "https://schema.org",
  "@type": "BlogPosting",
  "mainEntityOfPage": {
    "@type": "WebPage",
    "@id": "https://graphdeeplearning.github.io/post/transformers-are-gnns/"
  },
  "headline": "Transformers are Graph Neural Networks",
  
  "image": [
    "https://graphdeeplearning.github.io/post/transformers-are-gnns/featured.jpeg"
  ],
  
  "datePublished": "2020-02-12T16:08:39+08:00",
  "dateModified": "2020-02-12T16:08:39+08:00",
  
  "author": {
    "@type": "Person",
    "name": "Chaitanya Joshi"
  },
  
  "publisher": {
    "@type": "Organization",
    "name": "NTU Graph Deep Learning Lab",
    "logo": {
      "@type": "ImageObject",
      "url": "img/https://graphdeeplearning.github.io/"
    }
  },
  "description": "Engineer friends often ask me: Graph Deep Learning sounds great, but are there any big commercial success stories? Is it being deployed in practical applications?\nBesides the obvious ones\u0026ndash;recommendation systems at Pinterest, Alibaba and Twitter\u0026ndash;a slightly nuanced success story is the Transformer architecture, which has taken the NLP industry by storm.\nThrough this post, I want to establish links between Graph Neural Networks (GNNs) and Transformers. I\u0026rsquo;ll talk about the intuitions behind model architectures in the NLP and GNN communities, make connections using equations and figures, and discuss how we could work together to drive progress."
}
</script>

  

  


  


  





  <title>Transformers are Graph Neural Networks | NTU Graph Deep Learning Lab</title>

</head>

<body id="top" data-spy="scroll" data-offset="70" data-target="#TableOfContents" >

  <aside class="search-results" id="search">
  <div class="container">
    <section class="search-header">

      <div class="row no-gutters justify-content-between mb-3">
        <div class="col-6">
          <h1>Search</h1>
        </div>
        <div class="col-6 col-search-close">
          <a class="js-search" href="#"><i class="fas fa-times-circle text-muted" aria-hidden="true"></i></a>
        </div>
      </div>

      <div id="search-box">
        
        <input name="q" id="search-query" placeholder="Search..." autocapitalize="off"
        autocomplete="off" autocorrect="off" spellcheck="false" type="search">
        
      </div>

    </section>
    <section class="section-search-results">

      <div id="search-hits">
        
      </div>

    </section>
  </div>
</aside>


  

<nav class="navbar navbar-expand-lg navbar-light compensate-for-scrollbar" id="navbar-main">
  <div class="container">

    
    
    <div class="d-none d-lg-inline-flex">
      <a class="navbar-brand" href="/">NTU Graph Deep Learning Lab</a>
    </div>
    

    
    <button type="button" class="navbar-toggler" data-toggle="collapse"
            data-target="#navbar-content" aria-controls="navbar" aria-expanded="false" aria-label="Toggle navigation">
    <span><i class="fas fa-bars"></i></span>
    </button>
    

    
    <div class="navbar-brand-mobile-wrapper d-inline-flex d-lg-none">
      <a class="navbar-brand" href="/">NTU Graph Deep Learning Lab</a>
    </div>
    

    
    
    <div class="navbar-collapse main-menu-item collapse justify-content-end" id="navbar-content">

      
      <ul class="navbar-nav d-md-inline-flex">
        

        

        
        
        
          
        

        
        
        
        
        
        
          
          
          
            
          
          
        

        <li class="nav-item">
          <a class="nav-link " href="/#hero"><span>Home</span></a>
        </li>

        
        

        

        
        
        
          
        

        
        
        
        
        
        

        <li class="nav-item">
          <a class="nav-link " href="/publication"><span>Publications</span></a>
        </li>

        
        

        

        
        
        
          
        

        
        
        
        
        
        

        <li class="nav-item">
          <a class="nav-link  active" href="/post/"><span>Blog</span></a>
        </li>

        
        

        

        
        
        
          
        

        
        
        
        
        
        
          
          
          
            
          
          
        

        <li class="nav-item">
          <a class="nav-link " href="/#people"><span>People</span></a>
        </li>

        
        

        

        
        
        
          
        

        
        
        
        
        
        
          
          
          
            
          
          
        

        <li class="nav-item">
          <a class="nav-link " href="/#contact"><span>Contact</span></a>
        </li>

        
        

      

        
      </ul>
    </div>

    <ul class="nav-icons navbar-nav flex-row ml-auto d-flex pl-md-2">
      
      <li class="nav-item">
        <a class="nav-link js-search" href="#"><i class="fas fa-search" aria-hidden="true"></i></a>
      </li>
      

      

      

    </ul>

  </div>
</nav>


  <article class="article">

  












  

  
  
  
<div class="article-container pt-3">
  <h1>Transformers are Graph Neural Networks</h1>

  

  
    


<div class="article-metadata">

  
  
  
  
  <div>
    



  
  <span><a href="/authors/chaitanya-joshi/">Chaitanya Joshi</a></span>

  </div>
  
  

  
  <span class="article-date">
    
    
      
    
    Feb 12, 2020
  </span>
  

  

  

  
  
  

  
  

</div>

    













<div class="btn-links mb-3">
  
  








  









  
  <a class="btn btn-outline-primary my-1 mr-1" href="/project/spatial-convnets/">
    Project
  </a>
  

  
  <a class="btn btn-outline-primary my-1 mr-1" href="/project/sketches/">
    Project
  </a>
  





  
  
    
  
<a class="btn btn-outline-primary my-1 mr-1" href="/files/transformers-are-gnns-slides.pdf" target="_blank" rel="noopener">
  Slides
</a>






  
  
  
    
  
  
  
  
  
    
  
  <a class="btn btn-outline-primary my-1 mr-1" href="https://medium.com/@chaitjo/transformers-are-graph-neural-networks-bca9f75412aa?source=friends_link&amp;sk=c54de873b2cec3db70166a6cf0b41d3e" target="_blank" rel="noopener">
    
    Medium
  </a>


</div>


  
</div>



  <div class="article-container">

    <div class="article-style">
      <p>Engineer friends often ask me: Graph Deep Learning sounds great, but are there any big commercial success stories? Is it being deployed in practical applications?</p>
<p>Besides the obvious ones&ndash;recommendation systems at <a href="https://medium.com/pinterest-engineering/pinsage-a-new-graph-convolutional-neural-network-for-web-scale-recommender-systems-88795a107f48">Pinterest</a>, <a href="https://arxiv.org/abs/1902.08730">Alibaba</a> and <a href="https://blog.twitter.com/en_us/topics/company/2019/Twitter-acquires-Fabula-AI.html">Twitter</a>&ndash;a slightly nuanced success story is the <a href="https://arxiv.org/abs/1706.03762"><strong>Transformer architecture</strong></a>, which has <a href="https://openai.com/blog/better-language-models/">taken</a> <a href="https://www.blog.google/products/search/search-language-understanding-bert/">the</a> <a href="https://www.microsoft.com/en-us/research/project/large-scale-pretraining-for-response-generation/">NLP</a> <a href="https://ai.facebook.com/blog/roberta-an-optimized-method-for-pretraining-self-supervised-nlp-systems/">industry</a> <a href="https://blog.einstein.ai/introducing-a-conditional-transformer-language-model-for-controllable-generation/">by</a> <a href="https://nv-adlr.github.io/MegatronLM">storm</a>.</p>
<p>Through this post, I want to establish links between <a href="https://graphdeeplearning.github.io/project/spatial-convnets/">Graph Neural Networks (GNNs)</a> and Transformers.
I&rsquo;ll talk about the intuitions behind model architectures in the NLP and GNN communities, make connections using equations and figures, and discuss how we could work together to drive progress.</p>
<p>Let&rsquo;s start by talking about the purpose of model architectures&ndash;<em>representation learning</em>.</p>
<hr>
<h3 id="representation-learning-for-nlp">Representation Learning for NLP</h3>
<p>At a high level, all neural network architectures build <em>representations</em> of input data as vectors/embeddings, which encode useful statistical and semantic information about the data.
These <em>latent</em> or <em>hidden</em> representations can then be used for performing something useful, such as classifying an image or translating a sentence.
The neural network <em>learns</em> to build better-and-better representations by receiving feedback, usually via error/loss functions.</p>
<p>For Natural Language Processing (NLP), conventionally, <strong>Recurrent Neural Networks</strong> (RNNs) build representations of each word in a sentence in a sequential manner, <em>i.e.</em>, <strong>one word at a time</strong>.
Intuitively, we can imagine an RNN layer as a conveyor belt, with the words being processed on it <em>autoregressively</em> from left to right.
At the end, we get a hidden feature for each word in the sentence, which we pass to the next RNN layer or use for our NLP tasks of choice.</p>
<blockquote>
<p>I highly recommend Chris Olah&rsquo;s legendary blog for recaps on <a href="http://colah.github.io/posts/2015-08-Understanding-LSTMs/">RNNs</a> and <a href="http://colah.github.io/posts/2014-07-NLP-RNNs-Representations/">representation learning</a> for NLP.</p>
</blockquote>













<figure>


  <a data-fancybox="" href="rnn-transf-nlp.jpg" >
<img data-src="rnn-transf-nlp.jpg" class="lazyload" alt="" width="100%" ></a>



</figure>

<p>Initially introduced for machine translation, <strong>Transformers</strong> have gradually replaced RNNs in mainstream NLP.
The architecture takes a fresh approach to representation learning: Doing away with recurrence entirely, Transformers build features of each word using an <a href="https://distill.pub/2016/augmented-rnns/">attention</a> <a href="https://lilianweng.github.io/lil-log/2018/06/24/attention-attention.html">mechanism</a> to figure out how important <strong>all the other words</strong> in the sentence are w.r.t. to the aforementioned word.
Knowing this, the word&rsquo;s updated features are simply the sum of linear transformations of the features of all the words, weighted by their importance.</p>
<blockquote>
<p>Back in 2017, this idea sounded very radical, because the NLP community was so used to the sequential&ndash;one-word-at-a-time&ndash;style of processing text with RNNs. The title of the paper probably added fuel to the fire!</p>
<p>For a recap, Yannic Kilcher made an excellent <a href="https://www.youtube.com/watch?v=iDulhoQ2pro">video overview</a>.</p>
</blockquote>
<hr>
<h3 id="breaking-down-the-transformer">Breaking down the Transformer</h3>
<p>Let&rsquo;s develop intuitions about the architecture by translating the previous paragraph into the language of mathematical symbols and vectors.
We update the hidden feature $h$ of the $i$'th word in a sentence $\mathcal{S}$ from layer $\ell$ to layer $\ell+1$ as follows:</p>
<p>$$
h_{i}^{\ell+1} = \text{Attention} \left( Q^{\ell} h_{i}^{\ell} \ , K^{\ell} h_{j}^{\ell} \ , V^{\ell} h_{j}^{\ell} \right),
$$</p>
<p>$$
i.e.,\ h_{i}^{\ell+1} = \sum_{j \in \mathcal{S}} w_{ij} \left( V^{\ell} h_{j}^{\ell} \right),
$$</p>
<p>$$
\text{where} \ w_{ij} = \text{softmax}_j \left( Q^{\ell} h_{i}^{\ell} \cdot  K^{\ell} h_{j}^{\ell} \right),
$$</p>
<p>where $j \in \mathcal{S}$ denotes the set of words in the sentence and $Q^{\ell}, K^{\ell}, V^{\ell}$ are learnable linear weights (denoting the <strong>Q</strong>uery, <strong>K</strong>ey and <strong>V</strong>alue for the attention computation, respectively).
The attention mechanism is performed parallelly for each word in the sentence to obtain their updated features in <em>one shot</em>&ndash;another plus point for Transformers over RNNs, which update features word-by-word.</p>
<p>We can understand the attention mechanism better through the following pipeline:</p>













<figure>


  <a data-fancybox="" href="attention-block.jpg" >
<img data-src="attention-block.jpg" class="lazyload" alt="" width="50%" ></a>



</figure>

<blockquote>
<p>Taking in the features of the word $h_{i}^{\ell}$ and the set of other words in the sentence ${ h_{j}^{\ell} ;\ \forall j \in \mathcal{S} }$, we compute the attention weights $w_{ij}$ for each pair $(i,j)$ through the dot-product, followed by a softmax across all $j$'s. Finally, we produce the updated word feature $h_{i}^{\ell+1}$ for word $i$ by summing over all ${ h_{j}^{\ell} }$'s weighted by their corresponding $w_{ij}$. Each word in the sentence parallelly undergoes the same pipeline to update its features.</p>
</blockquote>
<hr>
<h3 id="multi-head-attention-mechanism">Multi-head Attention mechanism</h3>
<p>Getting this dot-product attention mechanism to work proves to be tricky&ndash;bad random initializations can de-stabilize the learning process.
We can overcome this by parallelly performing multiple &lsquo;heads&rsquo; of attention and concatenating the result (with each head now having separate learnable  weights):</p>
<p>$$
h_{i}^{\ell+1} = \text{Concat} \left( \text{head}_1, \ldots, \text{head}_K \right) O^{\ell},
$$
$$
\text{head}_k = \text{Attention} \left(  Q^{k,\ell} h_{i}^{\ell} \ , K^{k, \ell} h_{j}^{\ell} \ , V^{k, \ell} h_{j}^{\ell} \right),
$$</p>
<p>where $Q^{k,\ell}, K^{k,\ell}, V^{k,\ell}$ are the learnable weights of the $k$'th attention head and $O^{\ell}$ is a down-projection to match the dimensions of $h_i^{\ell+1}$ and $h_i^{\ell}$ across layers.</p>
<p>Multiple heads allow the attention mechanism to essentially &lsquo;hedge its bets&rsquo;, looking at different transformations or aspects of the hidden features from the previous layer.
We&rsquo;ll talk more about this later.</p>
<hr>
<h3 id="scale-issues-and-the-feed-forward-sub-layer">Scale issues and the Feed-forward sub-layer</h3>
<p>A key issue motivating the final Transformer architecture is that the features for words <em>after</em> the attention mechanism might be at <strong>different scales</strong> or <strong>magnitudes</strong>:
(1) This can be due to some words having very sharp or very distributed attention weights $w_{ij}$ when summing over the features of the other words.
(2) At the individual feature/vector entries level, concatenating across multiple attention heads&ndash;each of which might output values at different scales&ndash;can lead to the entries of the final vector $h_{i}^{\ell+1}$ having a wide range of values.
Following conventional ML wisdom, it seems reasonable to add a <a href="https://nealjean.com/ml/neural-network-normalization/">normalization layer</a> into the pipeline.</p>
<p>Transformers overcome issue (2) with <a href="https://arxiv.org/abs/1607.06450"><strong>LayerNorm</strong></a>, which normalizes and learns an affine transformation at the feature level.
Additionally, <strong>scaling the dot-product</strong> attention by the square-root of the feature dimension helps counteract issue (1).</p>
<p>Finally, the authors propose another &lsquo;trick&rsquo; to control the scale issue: <strong>a position-wise 2-layer MLP</strong> with a special structure.
After the multi-head attention, they project $h_i^{\ell+1}$ to a (absurdly) higher dimension by a learnable weight, where it undergoes the ReLU non-linearity, and is then projected back to its original dimension followed by another normalization:</p>
<p>$$
h_i^{\ell+1} = \text{LN} \left( \text{MLP} \left( \text{LN} \left( h_i^{\ell+1} \right) \right) \right)
$$</p>
<blockquote>
<p>To be honest, I&rsquo;m not sure what the exact intuition behind the over-parameterized feed-forward sub-layer was and nobody seems to be asking questions about it, too! I suppose LayerNorm and scaled dot-products didn&rsquo;t completely solve the issues highlighted, so the big MLP is a sort of hack to re-scale the feature vectors independently of each other.</p>
<p><a href="mailto:chaitanya.joshi@ntu.edu.sg">Email me</a> if you know more!</p>
</blockquote>
<hr>
<p>The final picture of a Transformer layer looks like this:</p>













<figure>


  <a data-fancybox="" href="transformer-block.png" >
<img data-src="transformer-block.png" class="lazyload" alt="" width="60%" ></a>



</figure>

<p>The Transformer architecture is also extremely amenable to very deep networks, enabling the NLP community to <em><a href="https://arxiv.org/abs/1910.10683">scale</a> <a href="https://arxiv.org/abs/2001.08361">up</a></em> in terms of both model parameters and, by extension, data.
<strong>Residual connections</strong> between the inputs and outputs of each multi-head attention sub-layer and the feed-forward sub-layer are key for stacking Transformer layers (but omitted from the diagram for clarity).</p>
<hr>
<h3 id="gnns-build-representations-of-graphs">GNNs build representations of graphs</h3>
<p>Let&rsquo;s take a step away from NLP for a moment.</p>
<p>Graph Neural Networks (GNNs) or Graph Convolutional Networks (GCNs) build representations of nodes and edges in graph data.
They do so through <strong>neighbourhood aggregation</strong> (or message passing), where each node gathers features from its neighbours to update its representation of the <em>local</em> graph structure around it.
Stacking several GNN layers enables the model to propagate each node&rsquo;s features over the entire graph&ndash;from its neighbours to the neighbours&rsquo; neighbours, and so on.</p>













<figure>


  <a data-fancybox="" href="gnn-social-network.jpg" >
<img data-src="gnn-social-network.jpg" class="lazyload" alt="" width="100%" ></a>



</figure>

<blockquote>
<p>Take the example of this emoji social network: The node features produced by the GNN can be used for predictive tasks such as identifying the most influential members or proposing potential connections.</p>
</blockquote>
<p>In their most basic form, GNNs update the hidden features $h$ of node $i$ (for example, ðŸ˜†) at layer $\ell$ via a non-linear transformation of the node&rsquo;s own features $h_i^{\ell}$ added to the aggregation of features $h_j^{\ell}$ from each neighbouring node $j \in \mathcal{N}(i)$:</p>
<p>$$
h_{i}^{\ell+1} =  \sigma \Big( U^{\ell} h_{i}^{\ell} + \sum_{j \in \mathcal{N}(i)} \left( V^{\ell} h_{j}^{\ell} \right)  \Big),
$$</p>
<p>where $U^{\ell}, V^{\ell}$ are learnable weight matrices of the GNN layer and $\sigma$ is a non-linearity such as ReLU.
In the example, $\mathcal{N}$(ðŸ˜†) $=$ { ðŸ˜˜, ðŸ˜Ž, ðŸ˜œ, ðŸ¤© }.</p>
<p>The summation over the neighbourhood nodes $j \in \mathcal{N}(i)$ can be replaced by other input size-invariant <strong>aggregation functions</strong> such as simple mean/max or something more powerful, such as a weighted sum via an <a href="https://petar-v.com/GAT/"><strong>attention mechanism</strong></a>.</p>
<p>Does that sound familiar?</p>
<p>Maybe a pipeline will help make the connection:</p>













<figure>


  <a data-fancybox="" href="gnn-block.jpg" >
<img data-src="gnn-block.jpg" class="lazyload" alt="" width="50%" ></a>



</figure>

<div class="alert alert-note">
  <div>
    If we were to do multiple parallel heads of neighbourhood aggregation and replace summation over the neighbours $j$ with the attention mechanism, <em>i.e.</em>, a weighted sum, we&rsquo;d get the <b>Graph Attention Network</b> (GAT). Add normalization and the feed-forward MLP, and voila, we have a <b>Graph Transformer</b>!
  </div>
</div>
<hr>
<h3 id="sentences-are-fully-connected-word-graphs">Sentences are fully-connected word graphs</h3>
<p>To make the connection more explicit, consider a sentence as a fully-connected graph, where each word is connected to every other word.
Now, we can use a GNN to build features for each node (word) in the graph (sentence), which we can then perform NLP tasks with.</p>













<figure>


  <a data-fancybox="" href="gnn-nlp.jpg" >
<img data-src="gnn-nlp.jpg" class="lazyload" alt="" width="90%" ></a>



</figure>

<p>Broadly, this is what Transformers are doing: they are <strong>GNNs with multi-head attention</strong> as the neighbourhood aggregation function.
Whereas standard GNNs aggregate features from their local neighbourhood nodes $j \in \mathcal{N}(i)$,
Transformers for NLP treat the entire sentence $\mathcal{S}$ as the local neighbourhood, aggregating features from each word $j \in \mathcal{S}$ at each layer.</p>
<p>Importantly, various problem-specific tricks&ndash;such as position encodings, causal/masked aggregation, learning rate schedules and extensive pre-training&ndash;are essential for the success of Transformers but seldom seem in the GNN community.
At the same time, looking at Transformers from a GNN perspective could inspire us to get rid of a lot of the <em>bells and whistles</em> in the architecture.</p>
<hr>
<h3 id="what-can-we-learn-from-each-other">What can we learn from each other?</h3>
<p>Now that we&rsquo;ve established a connection between Transformers and GNNs, let me throw some ideas around&hellip;</p>
<h4 id="are-fully-connected-graphs-the-best-input-format-for-nlp">Are fully-connected graphs the best input format for NLP?</h4>
<p>Before statistical NLP and ML, linguists like Noam Chomsky focused on developing fomal theories of <a href="https://en.wikipedia.org/wiki/Syntactic_Structures">linguistic structure</a>, such as <strong>syntax trees/graphs</strong>.
<a href="https://arxiv.org/abs/1503.00075">Tree LSTMs</a> already tried this, but maybe Transformers/GNNs are better architectures for bringing the world of linguistic theory and statistical NLP closer?</p>













<figure>


  <a data-fancybox="" href="syntax-tree.png" >
<img data-src="syntax-tree.png" class="lazyload" alt="" width="40%" ></a>



</figure>

<h4 id="how-to-learn-long-term-dependencies">How to learn long-term dependencies?</h4>
<p>Another issue with fully-connected graphs is that they make learning very long-term dependencies between words difficult.
This is simply due to how the number of edges in the graph <strong>scales quadratically</strong> with the number of nodes, <em>i.e.</em>, in an $n$ word sentence, a Transformer/GNN would be doing computations over $n^2$ pairs of words. Things get out of hand for very large $n$.</p>
<p>The NLP community&rsquo;s perspective on the long sequences and dependencies problem is interesting:
Making the attention mechanism <a href="https://openai.com/blog/sparse-transformer/">sparse</a> or <a href="https://ai.facebook.com/blog/making-transformer-networks-simpler-and-more-efficient/">adaptive</a> in terms of input size, adding <a href="https://ai.googleblog.com/2019/01/transformer-xl-unleashing-potential-of.html">recurrence</a> or <a href="https://deepmind.com/blog/article/A_new_model_and_dataset_for_long-range_memory">compression</a> into each layer,
and using <a href="https://www.pragmatic.ml/reformer-deep-dive/">Locality Sensitive Hashing</a> for efficient attention
are all promising new ideas for better Transformers.</p>
<p>It would be interesting to see ideas from the GNN community thrown into the mix, <em>e.g.</em>, <a href="https://arxiv.org/abs/1911.04070">Binary Partitioning</a> for sentence <strong>graph sparsification</strong> seems like another exciting approach.</p>













<figure>


  <a data-fancybox="" href="long-term-depend.png" >
<img data-src="long-term-depend.png" class="lazyload" alt="" width="80%" ></a>



</figure>

<h4 id="are-transformers-learning-neural-syntax">Are Transformers learning &lsquo;neural syntax&rsquo;?</h4>
<p>There have been <a href="https://pair-code.github.io/interpretability/bert-tree/">several</a> <a href="https://arxiv.org/abs/1905.05950">interesting</a> <a href="https://arxiv.org/abs/1906.04341">papers</a> from the NLP community on what Transformers might be learning.
The basic premise is that performing attention on all word pairs in a sentence&ndash;with the purpose of identifying which pairs are the most interesting&ndash;enables Transformers to learn something like a <strong>task-specific syntax</strong>.
Different heads in the multi-head attention might also be &lsquo;looking&rsquo; at different syntactic properties.</p>
<p>In graph terms, by using GNNs on full graphs, can we recover the most important edges&ndash;and what they might entail&ndash;from how the GNN performs neighbourhood aggregation at each layer?
I&rsquo;m <a href="https://arxiv.org/abs/1909.07913">not so convinced</a> by this view yet.</p>













<figure>


  <a data-fancybox="" href="attention-heads.png" >
<img data-src="attention-heads.png" class="lazyload" alt="" width="100%" ></a>



</figure>

<h4 id="why-multiple-heads-of-attention-why-attention">Why multiple heads of attention? Why attention?</h4>
<p>I&rsquo;m more sympathetic to the optimization view of the multi-head mechanism&ndash;having multiple attention heads <strong>improves learning</strong> and overcomes <strong>bad random initializations</strong>.
For instance, <a href="https://lena-voita.github.io/posts/acl19_heads.html">these</a> <a href="https://arxiv.org/abs/1905.10650">papers</a> showed that Transformer heads can be &lsquo;pruned&rsquo; or removed <em>after</em> training without significant performance impact.</p>
<p>Multi-head neighbourhood aggregation mechanisms have also proven effective in GNNs, <em>e.g.</em>, GAT uses the same multi-head attention and <a href="https://arxiv.org/abs/1611.08402">MoNet</a> uses multiple <em>Gaussian kernels</em> for aggregating features.
Although invented to stabilize attention mechanisms, could the multi-head trick become standard for squeezing out extra model performance?</p>
<p>Conversely, GNNs with simpler aggregation functions such as sum or max do not require multiple aggregation heads for stable training.
Wouldn&rsquo;t it be nice for Transformers if we didn&rsquo;t have to compute pair-wise compatibilities between each word pair in the sentence?</p>
<p>Could Transformers benefit from ditching attention, altogether? Yann Dauphin and collaborators&rsquo; <a href="https://arxiv.org/abs/1705.03122">recent</a> <a href="https://arxiv.org/abs/1901.10430">work</a> suggests an alternative <strong>ConvNet architecture</strong>.
Transformers, too, might ultimately be doing <a href="http://jbcordonnier.com/posts/attention-cnn/">something</a> <a href="https://twitter.com/ChrSzegedy/status/1232148457810538496">similar</a> to ConvNets!</p>













<figure>


  <a data-fancybox="" href="attention-conv.png" >
<img data-src="attention-conv.png" class="lazyload" alt="" width="100%" ></a>



</figure>

<h4 id="why-is-training-transformers-so-hard">Why is training Transformers so hard?</h4>
<p>Reading new Transformer papers makes me feel that training these models requires something akin to <em>black magic</em> when determining the best <strong>learning rate schedule, warmup strategy</strong> and <strong>decay settings</strong>.
This could simply be because the models are so huge and the NLP tasks studied are so challenging.</p>
<p>But <a href="https://arxiv.org/abs/1906.01787">recent</a> <a href="https://arxiv.org/abs/1910.06764">results</a> <a href="https://arxiv.org/abs/2002.04745">suggest</a> that it could also be due to the specific permutation of normalization and residual connections within the architecture.</p>
<blockquote class="twitter-tweet"><p lang="en" dir="ltr">I enjoyed reading the new <a href="https://twitter.com/DeepMind?ref_src=twsrc%5Etfw">@DeepMind</a> Transformer paper, but why is training these models such dark magic? &quot;For word-based LM we used 16, 000 warmup steps with 500, 000 decay steps and sacrifice 9,000 goats.&quot;<a href="https://t.co/dP49GTa4ze">https://t.co/dP49GTa4ze</a> <a href="https://t.co/1K3Fx4s3M8">pic.twitter.com/1K3Fx4s3M8</a></p>&mdash; Chaitanya Joshi (@chaitjo) <a href="https://twitter.com/chaitjo/status/1229335421806501888?ref_src=twsrc%5Etfw">February 17, 2020</a></blockquote> <script async src="https://platform.twitter.com/widgets.js" charset="utf-8"></script>	
<p>At this point I&rsquo;m ranting, but this makes me sceptical: Do we really need multiple heads of expensive pair-wise attention, overparameterized MLP sub-layers, and complicated learning schedules?</p>
<p>Do we really need massive models with <a href="https://www.technologyreview.com/s/613630/training-a-single-ai-model-can-emit-as-much-carbon-as-five-cars-in-their-lifetimes/">massive carbon footprints</a>?</p>
<p>Shouldn&rsquo;t architectures with good <a href="https://arxiv.org/abs/1806.01261">inductive biases</a> for the task at hand be easier to train?</p>
<hr>
<h3 id="further-reading">Further Reading</h3>
<p>To dive deep into the Transformer architecture from an NLP perspective, check out these amazing blog posts: <a href="http://jalammar.github.io/illustrated-transformer/">The Illustrated Transformer</a> and <a href="http://nlp.seas.harvard.edu/2018/04/03/attention.html">The Annotated Transformer</a>.</p>
<p>Also, this blog isn&rsquo;t the first to link GNNs and Transformers:
Here&rsquo;s <a href="https://ipam.wistia.com/medias/1zgl4lq6nh">an excellent talk</a> by Arthur Szlam on the history and connection between Attention/Memory Networks, GNNs and Transformers.
Similarly, DeepMind&rsquo;s <a href="https://arxiv.org/abs/1806.01261">star-studded position paper</a> introduces the <em>Graph Networks</em> framework, unifying all these ideas.
For a code walkthrough, the DGL team has <a href="https://docs.dgl.ai/en/latest/tutorials/models/4_old_wines/7_transformer.html">a nice tutorial</a> on seq2seq as a graph problem and building Transformers as GNNs.</p>
<p><strong>In our next post, we&rsquo;ll be doing the reverse: using GNN architectures as Transformers for NLP (based on the Transformers library by <a href="https://github.com/huggingface/transformers">ðŸ¤— HuggingFace</a>).</strong></p>
<p>Finally, we wrote <a href="https://graphdeeplearning.github.io/publication/xu-2019-multi/">a recent paper</a> applying Transformers to sketch graphs. Do check it out!</p>
<hr>
<h4 id="updates">Updates</h4>
<p>The post is also available on <a href="https://medium.com/@chaitjo/transformers-are-graph-neural-networks-bca9f75412aa?source=friends_link&amp;sk=c54de873b2cec3db70166a6cf0b41d3e">Medium</a>, and has been translated to <a href="https://mp.weixin.qq.com/s/DABEcNf1hHahlZFMttiT2g">Chinese</a> and <a href="https://habr.com/ru/post/491576/">Russian</a>.
Do join the discussion on <a href="https://twitter.com/chaitjo/status/1233220586358181888?s=20">Twitter</a>, <a href="https://www.reddit.com/r/MachineLearning/comments/fb86mo/d_transformers_are_graph_neural_networks_blog/">Reddit</a> or <a href="https://news.ycombinator.com/item?id=22518263">HackerNews</a>!</p>
<blockquote class="twitter-tweet"><p lang="en" dir="ltr">Transformers are a special case of Graph Neural Networks. This may be obvious to some, but the following blog post does a good job at explaining these important concepts. <a href="https://t.co/H8LT2F7LqC">https://t.co/H8LT2F7LqC</a></p>&mdash; Oriol Vinyals (@OriolVinyalsML) <a href="https://twitter.com/OriolVinyalsML/status/1233783593626951681?ref_src=twsrc%5Etfw">February 29, 2020</a></blockquote> <script async src="https://platform.twitter.com/widgets.js" charset="utf-8"></script>

    </div>

    





<div class="article-tags">
  
  <a class="badge badge-light" href="/tags/deep-learning/">Deep Learning</a>
  
  <a class="badge badge-light" href="/tags/graph-neural-networks/">Graph Neural Networks</a>
  
  <a class="badge badge-light" href="/tags/transformer/">Transformer</a>
  
  <a class="badge badge-light" href="/tags/natural-language-processing/">Natural Language Processing</a>
  
</div>














  
  
    
  
  






  
  
  
  
  <div class="media author-card content-widget-hr">
    
      
      <img class="portrait mr-3" src="/authors/chaitanya-joshi/avatar_hua82d079e830c87393317dc595f4db49b_154794_250x250_fill_q90_lanczos_center.jpg" alt="Avatar">
    

    <div class="media-body">
      <h5 class="card-title"><a href="/authors/chaitanya-joshi/">Chaitanya Joshi</a></h5>
      <h6 class="card-subtitle">Research Assistant</h6>
      <p class="card-text">Chaitanya Joshi is a Research Assistant under Dr. Xavier Bresson at NTU, Singapore, applying Graph Neural Networks to Operations Research and Combinatorial Optimization.</p>
      <ul class="network-icon" aria-hidden="true">
  
    
    
    
      
    
    
    
    
    
    <li>
      <a href="mailto:chaitanya-joshi@ntu.edu.sg" >
        <i class="fas fa-envelope"></i>
      </a>
    </li>
  
    
    
    
      
    
    
    
    
    
      
    
    <li>
      <a href="https://twitter.com/chaitjo" target="_blank" rel="noopener">
        <i class="fab fa-twitter"></i>
      </a>
    </li>
  
    
    
    
      
    
    
    
    
    
      
    
    <li>
      <a href="https://github.com/chaitjo" target="_blank" rel="noopener">
        <i class="fab fa-github"></i>
      </a>
    </li>
  
    
    
    
      
    
    
    
    
    
      
    
    <li>
      <a href="http://linkedin.com/in/chaitjo" target="_blank" rel="noopener">
        <i class="fab fa-linkedin"></i>
      </a>
    </li>
  
    
    
    
      
    
    
    
    
    
      
    
    <li>
      <a href="https://medium.com/@chaitjo" target="_blank" rel="noopener">
        <i class="fab fa-medium"></i>
      </a>
    </li>
  
    
    
    
    
    
    
    
      
    
    <li>
      <a href="https://scholar.google.com/citations?hl=en&amp;user=cwxVFVgAAAAJ" target="_blank" rel="noopener">
        <i class="ai ai-google-scholar"></i>
      </a>
    </li>
  
    
    
    
      
    
    
    
    
    
      
    
    <li>
      <a href="https://chaitjo.github.io/" target="_blank" rel="noopener">
        <i class="fas fa-globe"></i>
      </a>
    </li>
  
</ul>

    </div>
  </div>









  
  
  <div class="article-widget content-widget-hr">
    <h3>Related</h3>
    <ul>
      
      <li><a href="/publication/xu-2019-multi/">Multi-Graph Transformer for Free-Hand Sketch Recognition</a></li>
      
      <li><a href="/publication/lakhotia-2018-experimental/">An Experimental Comparison of Text Classification Techniques</a></li>
      
      <li><a href="/project/sketches/">Free-hand Sketches</a></li>
      
      <li><a href="/talk/ipam-feb2018/">Convolutional Neural Networks on Graphs</a></li>
      
      <li><a href="/talk/ipam-sept2019/">Graph Convolutional Neural Networks for Molecule Generation</a></li>
      
    </ul>
  </div>
  



  </div>
</article>

      

    
    
    
      <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.4.1/jquery.min.js" integrity="sha256-CSXorXvZcTkaix6Yvo6HppcZGetbYMGWSFlBw8HfCJo=" crossorigin="anonymous"></script>
      <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery.imagesloaded/4.1.4/imagesloaded.pkgd.min.js" integrity="sha256-lqvxZrPLtfffUl2G/e7szqSvPBILGbwmsGE1MKlOi0Q=" crossorigin="anonymous"></script>
      <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery.isotope/3.0.6/isotope.pkgd.min.js" integrity="sha256-CBrpuqrMhXwcLLUd5tvQ4euBHCdh7wGlDfNz8vbu/iI=" crossorigin="anonymous"></script>
      <script src="https://cdnjs.cloudflare.com/ajax/libs/fancybox/3.5.7/jquery.fancybox.min.js" integrity="sha256-yt2kYMy0w8AbtF89WXb2P1rfjcP/HTHLT7097U8Y5b8=" crossorigin="anonymous"></script>

      

      
        
        <script src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/9.15.10/highlight.min.js" integrity="sha256-1zu+3BnLYV9LdiY85uXMzii3bdrkelyp37e0ZyTAQh0=" crossorigin="anonymous"></script>
        
        <script src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/9.15.10/languages/r.min.js"></script>
        
        <script src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/9.15.10/languages/python.min.js"></script>
        
      

    

    
    
      <script src="https://cdnjs.cloudflare.com/ajax/libs/leaflet/1.5.1/leaflet.js" integrity="sha256-EErZamuLefUnbMBQbsEqu1USa+btR2oIlCpBJbyD4/g=" crossorigin="anonymous"></script>
    

    
    
    <script>const code_highlighting = true;</script>
    

    
    
    
    
    
    
    <script>
      const search_config = {"indexURI":"/index.json","minLength":1,"threshold":0.3};
      const i18n = {"no_results":"No results found","placeholder":"Search...","results":"results found"};
      const content_type = {
        'post': "Posts",
        'project': "Projects",
        'publication' : "Publications",
        'talk' : "Talks"
        };
    </script>
    

    
    

    
    
    <script id="search-hit-fuse-template" type="text/x-template">
      <div class="search-hit" id="summary-{{key}}">
      <div class="search-hit-content">
        <div class="search-hit-name">
          <a href="{{relpermalink}}">{{title}}</a>
          <div class="article-metadata search-hit-type">{{type}}</div>
          <p class="search-hit-description">{{snippet}}</p>
        </div>
      </div>
      </div>
    </script>
    

    
    
    <script src="https://cdnjs.cloudflare.com/ajax/libs/fuse.js/3.2.1/fuse.min.js" integrity="sha256-VzgmKYmhsGNNN4Ph1kMW+BjoYJM2jV5i4IlFoeZA9XI=" crossorigin="anonymous"></script>
    <script src="https://cdnjs.cloudflare.com/ajax/libs/mark.js/8.11.1/jquery.mark.min.js" integrity="sha256-4HLtjeVgH0eIB3aZ9mLYF6E8oU5chNdjU6p6rrXpl9U=" crossorigin="anonymous"></script>
    

    
    

    
    

    
    
    
    
    
    
    
    
    
      
    
    
    
    
    <script src="/js/academic.min.a0d331bcd05dbe8b31e244f796710f08.js"></script>

    






  
  
  <div class="container">
    <footer class="site-footer">
  

  <p class="powered-by">
    Xavier Bresson Â© 2020 Â· Made with â™¥ by <a href="https://chaitjo.github.io/">Chaitanya Joshi</a> &middot; 

    Powered by the
    <a href="https://sourcethemes.com/academic/" target="_blank" rel="noopener">Academic theme</a> for
    <a href="https://gohugo.io" target="_blank" rel="noopener">Hugo</a>.

    
    <span class="float-right" aria-hidden="true">
      <a href="#" class="back-to-top">
        <span class="button_icon">
          <i class="fas fa-chevron-up fa-2x"></i>
        </span>
      </a>
    </span>
    
  </p>
</footer>

  </div>
  

  
<div id="modal" class="modal fade" role="dialog">
  <div class="modal-dialog">
    <div class="modal-content">
      <div class="modal-header">
        <h5 class="modal-title">Cite</h5>
        <button type="button" class="close" data-dismiss="modal" aria-label="Close">
          <span aria-hidden="true">&times;</span>
        </button>
      </div>
      <div class="modal-body">
        <pre><code class="tex hljs"></code></pre>
      </div>
      <div class="modal-footer">
        <a class="btn btn-outline-primary my-1 js-copy-cite" href="#" target="_blank">
          <i class="fas fa-copy"></i> Copy
        </a>
        <a class="btn btn-outline-primary my-1 js-download-cite" href="#" target="_blank">
          <i class="fas fa-download"></i> Download
        </a>
        <div id="modal-error"></div>
      </div>
    </div>
  </div>
</div>

</body>
</html>
